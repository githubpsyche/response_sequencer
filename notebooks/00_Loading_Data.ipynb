{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBS Narrative Recall Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export: null\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class SBS_NarrativeDataset(Dataset):\n",
    "    def __init__(self, data_directory):\n",
    "        \n",
    "        # each response file represents a sample; these are named based on '{story_name}_{subject_id}_{iteration}'\n",
    "        self.data_directory = data_directory\n",
    "        self.text_directory = os.path.join(data_directory, 'texts')\n",
    "        self.sequence_directory = os.path.join(data_directory, 'sequences', 'human')\n",
    "        self.response_files = []\n",
    "        self.stories = {}\n",
    "        for path, subdirs, files in os.walk(self.text_directory):\n",
    "            for name in files:\n",
    "                if name.count('_') == 2:\n",
    "                    self.response_files.append(name)\n",
    "                else:\n",
    "                    with open(os.path.join(path, name), 'r', encoding='utf-8') as f:\n",
    "                        self.stories[name[:-4]] = f.read()\n",
    "\n",
    "        self.trial_indices = np.arange(len(self.response_files))\n",
    "\n",
    "    def _retrieve_story_text(self, response_file_name):\n",
    "        story_name = response_file_name.split('_')[0]\n",
    "        return self.stories[story_name]\n",
    "\n",
    "    def _retrieve_response_text(self, response_file_name):\n",
    "        story_name = response_file_name.split('_')[0]\n",
    "        with open(os.path.join(self.text_directory, story_name, response_file_name), 'r') as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def _retrieve_response_sequence(self, response_file_name):\n",
    "        story_name = response_file_name.split('_')[0]\n",
    "        with open(os.path.join(self.sequence_directory, story_name, response_file_name[:-3]+'json'), 'r') as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def _prepare_match_matrix(self, response_sequence):\n",
    "        matchings = response_sequence['correspondences']\n",
    "        match_matrix = np.zeros(\n",
    "            (len(response_sequence['source_units']), len(response_sequence['response_units'])), dtype=bool)\n",
    "\n",
    "        for response_index, matched_target in enumerate(matchings):\n",
    "            if matched_target > -1:\n",
    "                match_matrix[matched_target, response_index] = True\n",
    "        return match_matrix\n",
    "    \n",
    "    def _prepare_target_items(self, story_text, response_sequence):\n",
    "        updated_unit_start = [story_text.find(unit) for unit in response_sequence['source_units']]\n",
    "        \n",
    "        for index, unit in enumerate(response_sequence['source_units']):\n",
    "            assert(updated_unit_start[index] > -1)\n",
    "\n",
    "        return [{'text': unit, 'spans':[(start, start+len(unit))]} for unit, start in zip(\n",
    "            response_sequence['source_units'], updated_unit_start)]\n",
    "    \n",
    "    def _prepare_response_units(self, response_text, response_sequence):\n",
    "        updated_text = [\n",
    "            response_text[span[0]:span[1]].strip() for span in response_sequence['response_spans']]\n",
    "\n",
    "        return [{'text': unit, 'spans':[(span[0], span[0]+len(unit))]} for unit, span in zip(\n",
    "            updated_text, response_sequence['response_spans'])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.response_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        trials = []\n",
    "        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n",
    "\n",
    "            response_file_name = self.response_files[trial_idx]\n",
    "            story_text = self._retrieve_story_text(response_file_name)\n",
    "            response_text = self._retrieve_response_text(response_file_name)\n",
    "            response_sequence = self._retrieve_response_sequence(response_file_name)\n",
    "\n",
    "            trials.append({\n",
    "                'target_context': story_text, \n",
    "                'target_items': self._prepare_target_items(story_text, response_sequence), \n",
    "                'response_transcript': response_text, \n",
    "                'response_units': self._prepare_response_units(response_text, response_sequence), \n",
    "                'matches': self._prepare_match_matrix(response_sequence)})\n",
    "            \n",
    "        return trials[0] if len(trials) == 1 else trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'C:/Users/gunnj/compmempy/data/narrative'\n",
    "dataset = SBS_NarrativeDataset(data_directory)\n",
    "\n",
    "example_entry = dataset[0]\n",
    "example_entry"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export: null\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import chardet\n",
    "import hdf5storage\n",
    "\n",
    "class SensesDataset(Dataset):\n",
    "    def __init__(self, hdf5_file_path, sense_pool_path, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            hdf5_file_path (str): Path to the HDF5 file.\n",
    "            sense_pool_path (str): Path to the sense pool file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Load the data from the specified HDF5 file\n",
    "        # You can customize this part to load your data\n",
    "        self.data = hdf5storage.read(path='/data', filename=hdf5_file_path)\n",
    "\n",
    "        self.trial_count = len(self.data[\"subject\"])\n",
    "        self.trial_indices = np.arange(self.trial_count)\n",
    "        with open(sense_pool_path, mode='rb') as f:\n",
    "            raw_data = f.read()\n",
    "            detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "            self.sense_pool = raw_data.decode(detected_encoding).split('\\n')\n",
    "        \n",
    "        # Set the transform if provided\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of trials in the dataset.\n",
    "        \"\"\"\n",
    "        return self.trial_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a trial from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            trial_idx (int): Index of the trial to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            sample: The retrieved sample.\n",
    "        \"\"\"\n",
    "\n",
    "        # pres_itemids selects the indices from sense_pool of the target items\n",
    "        trials = []\n",
    "        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n",
    "            senses = self.data['pres_itemids'][trial_idx]\n",
    "            target_items = [self.sense_pool[each-1].strip() for each in senses if each != 0]\n",
    "            \n",
    "            # response units are the segments of the transcript selected by raters and their spans in the transcript\n",
    "            response_transcript = str(self.data['recall_transcript'][trial_idx][0])\n",
    "            response_units = [str(each) for each in self.data['response_units'][trial_idx] if str(each) != '']\n",
    "\n",
    "            # full response units include text *and* span representations contained in a dict\n",
    "            response_start_spans = [each-1 for each in self.data[\"response_unit_start\"][trial_idx] if each != 0]\n",
    "            response_end_spans = [each-1 for each in self.data[\"response_unit_end\"][trial_idx] if each != 0]\n",
    "            full_response_units = [{'text': unit, 'spans':[(start_span, end_span)]} for unit, start_span, end_span in zip(\n",
    "                response_units, response_start_spans, response_end_spans)]\n",
    "            \n",
    "            # matchings\n",
    "            match_matrix = np.zeros((len(target_items), len(response_units)), dtype=bool)\n",
    "            target_indices = np.array([each-1 for each in self.data['recalls'][trial_idx] if each != 0])\n",
    "\n",
    "            if len(target_indices) > 0:\n",
    "                match_matrix[target_indices, np.arange(len(response_units))] = True\n",
    "\n",
    "            trials.append({\n",
    "                'target_context': '', \n",
    "                'target_items': target_items, \n",
    "                'response_transcript': response_transcript, \n",
    "                'response_units': full_response_units, \n",
    "                'matches': match_matrix})\n",
    "            \n",
    "        return trials[0] if len(trials) == 1 else trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_tag = 'base' # unique identifier for this variation of notebook parameters\n",
    "output_dir = 'C:/Users/gunnj/workspace/response_sequencer/data/'\n",
    "\n",
    "dataset = SensesDataset(os.path.join(output_dir, f'{section_tag}_senses.h5'), os.path.join(output_dir, f'{section_tag}_sense_pool.txt'))\n",
    "\n",
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide: null\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
