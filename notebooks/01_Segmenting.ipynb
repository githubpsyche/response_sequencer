{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenter\n",
    "The Segmenter abstract class defines the interface for implementations that segment raw response text into response units. Concrete implementations of this class can use various approaches such as rule-based, machine learning, or natural language processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp segmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class Segmenter(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for implementing text segmentation strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, text: str) -> List[Dict[str, object]]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of response units.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, object]]: The list of segmented response units.\n",
    "            \n",
    "            Each unit is a dictionary with keys \"text\" and \"spans\":\n",
    "            The \"text\" key corresponds to the text representation, \n",
    "            The \"spans\" key corresponds to a list of character-level start and end indices (tuples) in the input text.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Segmenter\n",
    "A baseline concrete class that applies `spacy`'s built-in sentence tokenizer to segment text into response units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import spacy\n",
    "\n",
    "class SentenceSegmenter(Segmenter):\n",
    "\n",
    "    \"\"\"\n",
    "    Concrete Segmenter class that identifies the sentences in the input text using the SpaCy library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def __call__(self, text: str) -> List[Dict[str, object]]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of response units.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, object]]: The list of segmented response units.\n",
    "            \n",
    "            Each unit is a dictionary with keys \"text\" and \"spans\":\n",
    "            The \"text\" key corresponds to the text representation, \n",
    "            The \"spans\" key corresponds to a list of character-level start and end indices (tuples) in the input text.\n",
    "        \"\"\"\n",
    "        return [{\"text\": sent.text, \"spans\": [(sent.start_char, sent.end_char)]} for sent in self._nlp(text).sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'One fine day an old Maine man was fishing on his favorite lake and catching very little.',\n",
       "  'spans': [(0, 88)]},\n",
       " {'text': 'Finally, he gave up and walked back along the shore to his fishing shack.',\n",
       "  'spans': [(89, 162)]},\n",
       " {'text': 'When he got close to the front door, he saw it was open.',\n",
       "  'spans': [(163, 219)]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter = SentenceSegmenter()\n",
    "\n",
    "text = \"One fine day an old Maine man was fishing on his favorite lake and catching very little. Finally, he gave up and walked back along the shore to his fishing shack. When he got close to the front door, he saw it was open. \"\n",
    "result = segmenter(text)\n",
    "\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllSentenceFragmentsSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import spacy\n",
    "from typing import List\n",
    "from spacy.tokens import Span\n",
    "\n",
    "class AllSentenceFragmentsSegmenter(Segmenter):\n",
    "\n",
    "    \"\"\"\n",
    "    Concrete Segmenter class that generates all possible segments of tokens in each sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def __call__(self, text: str) -> List[List[Tuple[str, Tuple[int, int]]]]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of response units.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, object]]: The list of segmented response units.\n",
    "            \n",
    "            Each unit is a dictionary with keys \"text\" and \"spans\":\n",
    "            The \"text\" key corresponds to the text representation, \n",
    "            The \"spans\" key corresponds to a list of character-level start and end indices (tuples) in the input text.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        for sentence in self.nlp(text).sents:\n",
    "            for i in range(len(sentence)):\n",
    "                for j in range(i + 1, len(sentence) + 1):\n",
    "\n",
    "                    sentence_fragment = sentence[i:j]\n",
    "                    if sentence_fragment.text.strip():\n",
    "                        segments.append({\n",
    "                            \"text\": sentence_fragment.text,\n",
    "                            \"spans\": [(sentence_fragment.start_char, sentence_fragment.end_char)]\n",
    "                        })\n",
    "\n",
    "        return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'One', 'spans': [(0, 3)]},\n",
       " {'text': 'One fine', 'spans': [(0, 8)]},\n",
       " {'text': 'One fine day', 'spans': [(0, 12)]},\n",
       " {'text': 'One fine day an', 'spans': [(0, 15)]},\n",
       " {'text': 'One fine day an old', 'spans': [(0, 19)]},\n",
       " {'text': 'One fine day an old Maine', 'spans': [(0, 25)]},\n",
       " {'text': 'One fine day an old Maine man', 'spans': [(0, 29)]},\n",
       " {'text': 'One fine day an old Maine man was', 'spans': [(0, 33)]},\n",
       " {'text': 'One fine day an old Maine man was fishing', 'spans': [(0, 41)]},\n",
       " {'text': 'fine', 'spans': [(4, 8)]},\n",
       " {'text': 'fine day', 'spans': [(4, 12)]},\n",
       " {'text': 'fine day an', 'spans': [(4, 15)]},\n",
       " {'text': 'fine day an old', 'spans': [(4, 19)]},\n",
       " {'text': 'fine day an old Maine', 'spans': [(4, 25)]},\n",
       " {'text': 'fine day an old Maine man', 'spans': [(4, 29)]},\n",
       " {'text': 'fine day an old Maine man was', 'spans': [(4, 33)]},\n",
       " {'text': 'fine day an old Maine man was fishing', 'spans': [(4, 41)]},\n",
       " {'text': 'day', 'spans': [(9, 12)]},\n",
       " {'text': 'day an', 'spans': [(9, 15)]},\n",
       " {'text': 'day an old', 'spans': [(9, 19)]},\n",
       " {'text': 'day an old Maine', 'spans': [(9, 25)]},\n",
       " {'text': 'day an old Maine man', 'spans': [(9, 29)]},\n",
       " {'text': 'day an old Maine man was', 'spans': [(9, 33)]},\n",
       " {'text': 'day an old Maine man was fishing', 'spans': [(9, 41)]},\n",
       " {'text': 'an', 'spans': [(13, 15)]},\n",
       " {'text': 'an old', 'spans': [(13, 19)]},\n",
       " {'text': 'an old Maine', 'spans': [(13, 25)]},\n",
       " {'text': 'an old Maine man', 'spans': [(13, 29)]},\n",
       " {'text': 'an old Maine man was', 'spans': [(13, 33)]},\n",
       " {'text': 'an old Maine man was fishing', 'spans': [(13, 41)]},\n",
       " {'text': 'old', 'spans': [(16, 19)]},\n",
       " {'text': 'old Maine', 'spans': [(16, 25)]},\n",
       " {'text': 'old Maine man', 'spans': [(16, 29)]},\n",
       " {'text': 'old Maine man was', 'spans': [(16, 33)]},\n",
       " {'text': 'old Maine man was fishing', 'spans': [(16, 41)]},\n",
       " {'text': 'Maine', 'spans': [(20, 25)]},\n",
       " {'text': 'Maine man', 'spans': [(20, 29)]},\n",
       " {'text': 'Maine man was', 'spans': [(20, 33)]},\n",
       " {'text': 'Maine man was fishing', 'spans': [(20, 41)]},\n",
       " {'text': 'man', 'spans': [(26, 29)]},\n",
       " {'text': 'man was', 'spans': [(26, 33)]},\n",
       " {'text': 'man was fishing', 'spans': [(26, 41)]},\n",
       " {'text': 'was', 'spans': [(30, 33)]},\n",
       " {'text': 'was fishing', 'spans': [(30, 41)]},\n",
       " {'text': 'fishing', 'spans': [(34, 41)]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"One fine day an old Maine man was fishing \"\n",
    "segmenter = AllSentenceFragmentsSegmenter()\n",
    "segmenter(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiSentenceFragmentSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import spacy\n",
    "from typing import List, Tuple\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "class MultiSentenceFragmentsSegmenter(Segmenter):\n",
    "\n",
    "    \"\"\"\n",
    "    Concrete Segmenter class that generates all possible segments of tokens\n",
    "    within a specified number of sentences in the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_sentences: int, min_tokens: int = 1):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.nlp.Defaults.stop_words.add(\"um\")\n",
    "        self.nlp.Defaults.stop_words.add(\"uh\")\n",
    "\n",
    "        self.max_sentences = max_sentences\n",
    "        self.min_tokens = min_tokens\n",
    "\n",
    "    def __call__(self, text: str) -> List[Dict[str, object]]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of response units.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, object]]: The list of segmented response units.\n",
    "            \n",
    "            Each unit is a dictionary with keys \"text\" and \"spans\":\n",
    "            The \"text\" key corresponds to the text representation, \n",
    "            The \"spans\" key corresponds to a list of character-level start and end indices (tuples) in the input text.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        doc = self.nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        num_sentences = len(sentences)\n",
    "\n",
    "        for start_idx in range(num_sentences):\n",
    "            for end_idx in range(start_idx + 1, min(start_idx + self.max_sentences + 1, num_sentences + 1)):\n",
    "                segment = doc[sentences[start_idx].start:sentences[end_idx - 1].end]\n",
    "\n",
    "                for i in range(len(segment)):\n",
    "                    for j in range(i + 1, len(segment) + 1):\n",
    "\n",
    "                        fragment = segment[i:j]\n",
    "\n",
    "                        token_count = sum(not (token.is_punct or token.is_stop) for token in fragment)\n",
    "\n",
    "                        if token_count >= self.min_tokens and fragment.text.strip():\n",
    "                            segments.append({\n",
    "                                \"text\": fragment.text,\n",
    "                                \"spans\": [(fragment.start_char, fragment.end_char)]\n",
    "                            })\n",
    "\n",
    "        return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'This', 'spans': [(0, 4)]},\n",
       " {'text': 'This is', 'spans': [(0, 7)]},\n",
       " {'text': 'This is the', 'spans': [(0, 11)]},\n",
       " {'text': 'This is the first', 'spans': [(0, 17)]},\n",
       " {'text': 'This is the first sentence', 'spans': [(0, 26)]},\n",
       " {'text': 'This is the first sentence.', 'spans': [(0, 27)]},\n",
       " {'text': 'is', 'spans': [(5, 7)]},\n",
       " {'text': 'is the', 'spans': [(5, 11)]},\n",
       " {'text': 'is the first', 'spans': [(5, 17)]},\n",
       " {'text': 'is the first sentence', 'spans': [(5, 26)]},\n",
       " {'text': 'is the first sentence.', 'spans': [(5, 27)]},\n",
       " {'text': 'the', 'spans': [(8, 11)]},\n",
       " {'text': 'the first', 'spans': [(8, 17)]},\n",
       " {'text': 'the first sentence', 'spans': [(8, 26)]},\n",
       " {'text': 'the first sentence.', 'spans': [(8, 27)]},\n",
       " {'text': 'first', 'spans': [(12, 17)]},\n",
       " {'text': 'first sentence', 'spans': [(12, 26)]},\n",
       " {'text': 'first sentence.', 'spans': [(12, 27)]},\n",
       " {'text': 'sentence', 'spans': [(18, 26)]},\n",
       " {'text': 'sentence.', 'spans': [(18, 27)]},\n",
       " {'text': 'THAT', 'spans': [(28, 32)]},\n",
       " {'text': 'THAT IS', 'spans': [(28, 35)]},\n",
       " {'text': 'THAT IS THE', 'spans': [(28, 39)]},\n",
       " {'text': 'THAT IS THE SECOND', 'spans': [(28, 46)]},\n",
       " {'text': 'THAT IS THE SECOND SENTENCE', 'spans': [(28, 55)]},\n",
       " {'text': 'THAT IS THE SECOND SENTENCE.', 'spans': [(28, 56)]},\n",
       " {'text': 'IS', 'spans': [(33, 35)]},\n",
       " {'text': 'IS THE', 'spans': [(33, 39)]},\n",
       " {'text': 'IS THE SECOND', 'spans': [(33, 46)]},\n",
       " {'text': 'IS THE SECOND SENTENCE', 'spans': [(33, 55)]},\n",
       " {'text': 'IS THE SECOND SENTENCE.', 'spans': [(33, 56)]},\n",
       " {'text': 'THE', 'spans': [(36, 39)]},\n",
       " {'text': 'THE SECOND', 'spans': [(36, 46)]},\n",
       " {'text': 'THE SECOND SENTENCE', 'spans': [(36, 55)]},\n",
       " {'text': 'THE SECOND SENTENCE.', 'spans': [(36, 56)]},\n",
       " {'text': 'SECOND', 'spans': [(40, 46)]},\n",
       " {'text': 'SECOND SENTENCE', 'spans': [(40, 55)]},\n",
       " {'text': 'SECOND SENTENCE.', 'spans': [(40, 56)]},\n",
       " {'text': 'SENTENCE', 'spans': [(47, 55)]},\n",
       " {'text': 'SENTENCE.', 'spans': [(47, 56)]},\n",
       " {'text': 'HeRe', 'spans': [(57, 61)]},\n",
       " {'text': 'HeRe Is', 'spans': [(57, 64)]},\n",
       " {'text': 'HeRe Is ThE', 'spans': [(57, 68)]},\n",
       " {'text': 'HeRe Is ThE ThIrD', 'spans': [(57, 74)]},\n",
       " {'text': 'HeRe Is ThE ThIrD SeNtEnCe', 'spans': [(57, 83)]},\n",
       " {'text': 'HeRe Is ThE ThIrD SeNtEnCe.', 'spans': [(57, 84)]},\n",
       " {'text': 'Is', 'spans': [(62, 64)]},\n",
       " {'text': 'Is ThE', 'spans': [(62, 68)]},\n",
       " {'text': 'Is ThE ThIrD', 'spans': [(62, 74)]},\n",
       " {'text': 'Is ThE ThIrD SeNtEnCe', 'spans': [(62, 83)]},\n",
       " {'text': 'Is ThE ThIrD SeNtEnCe.', 'spans': [(62, 84)]},\n",
       " {'text': 'ThE', 'spans': [(65, 68)]},\n",
       " {'text': 'ThE ThIrD', 'spans': [(65, 74)]},\n",
       " {'text': 'ThE ThIrD SeNtEnCe', 'spans': [(65, 83)]},\n",
       " {'text': 'ThE ThIrD SeNtEnCe.', 'spans': [(65, 84)]},\n",
       " {'text': 'ThIrD', 'spans': [(69, 74)]},\n",
       " {'text': 'ThIrD SeNtEnCe', 'spans': [(69, 83)]},\n",
       " {'text': 'ThIrD SeNtEnCe.', 'spans': [(69, 84)]},\n",
       " {'text': 'SeNtEnCe', 'spans': [(75, 83)]},\n",
       " {'text': 'SeNtEnCe.', 'spans': [(75, 84)]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is the first sentence. THAT IS THE SECOND SENTENCE. HeRe Is ThE ThIrD SeNtEnCe.\"\n",
    "segmenter = MultiSentenceFragmentsSegmenter(max_sentences=1, min_tokens=1)\n",
    "segmenter(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AllFragmentsSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import spacy\n",
    "from typing import List, Tuple\n",
    "\n",
    "class AllFragmentsSegmenter:\n",
    "    \"\"\"\n",
    "    Concrete Segmenter class that generates all possible segments of tokens in the entire text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def __call__(self, text: str) -> List[Tuple[str, Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of response units.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[str, Tuple[int, int]]]: The list of segmented response units.\n",
    "            \n",
    "            Each unit is a tuple with the following elements:\n",
    "            - The text representation of the fragment.\n",
    "            - A tuple representing the character-level start and end indices of the fragment in the input text.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # Iterate over all possible fragments of tokens in the entire text\n",
    "        for i in range(len(doc)):\n",
    "            for j in range(i + 1, len(doc) + 1):\n",
    "                fragment = doc[i:j]\n",
    "                if fragment.text.strip():\n",
    "                    segments.append({'text': fragment.text, 'spans':[(fragment.start_char, fragment.end_char)]})\n",
    "\n",
    "        return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('One', [(0, 3)]),\n",
       " ('One fine', [(0, 8)]),\n",
       " ('One fine day', [(0, 12)]),\n",
       " ('One fine day an', [(0, 15)]),\n",
       " ('One fine day an old', [(0, 19)]),\n",
       " ('One fine day an old Maine', [(0, 25)]),\n",
       " ('One fine day an old Maine man', [(0, 29)]),\n",
       " ('One fine day an old Maine man was', [(0, 33)]),\n",
       " ('One fine day an old Maine man was fishing', [(0, 41)]),\n",
       " ('fine', [(4, 8)]),\n",
       " ('fine day', [(4, 12)]),\n",
       " ('fine day an', [(4, 15)]),\n",
       " ('fine day an old', [(4, 19)]),\n",
       " ('fine day an old Maine', [(4, 25)]),\n",
       " ('fine day an old Maine man', [(4, 29)]),\n",
       " ('fine day an old Maine man was', [(4, 33)]),\n",
       " ('fine day an old Maine man was fishing', [(4, 41)]),\n",
       " ('day', [(9, 12)]),\n",
       " ('day an', [(9, 15)]),\n",
       " ('day an old', [(9, 19)]),\n",
       " ('day an old Maine', [(9, 25)]),\n",
       " ('day an old Maine man', [(9, 29)]),\n",
       " ('day an old Maine man was', [(9, 33)]),\n",
       " ('day an old Maine man was fishing', [(9, 41)]),\n",
       " ('an', [(13, 15)]),\n",
       " ('an old', [(13, 19)]),\n",
       " ('an old Maine', [(13, 25)]),\n",
       " ('an old Maine man', [(13, 29)]),\n",
       " ('an old Maine man was', [(13, 33)]),\n",
       " ('an old Maine man was fishing', [(13, 41)]),\n",
       " ('old', [(16, 19)]),\n",
       " ('old Maine', [(16, 25)]),\n",
       " ('old Maine man', [(16, 29)]),\n",
       " ('old Maine man was', [(16, 33)]),\n",
       " ('old Maine man was fishing', [(16, 41)]),\n",
       " ('Maine', [(20, 25)]),\n",
       " ('Maine man', [(20, 29)]),\n",
       " ('Maine man was', [(20, 33)]),\n",
       " ('Maine man was fishing', [(20, 41)]),\n",
       " ('man', [(26, 29)]),\n",
       " ('man was', [(26, 33)]),\n",
       " ('man was fishing', [(26, 41)]),\n",
       " ('was', [(30, 33)]),\n",
       " ('was fishing', [(30, 41)]),\n",
       " ('fishing', [(34, 41)])]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"One fine day an old Maine man was fishing \"\n",
    "segmenter = AllFragmentsSegmenter()\n",
    "segmenter(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClausiePropositionSegmenter\n",
    "The `ClausiePropositionSegmenter` class uses the `spacy-clausie` library to segment sentences into propositions. `clausie` uses a rule-based approach to identify propositions within sentences. It uses a set of hand-crafted rules to identify the subject, verb, and object of a proposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import spacy\n",
    "import claucy\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ClausiePropositionSegmenter(Segmenter):\n",
    "\n",
    "    \"\"\"\n",
    "    Concrete Segmenter class that generates propositions from sentences using spacy-clausie.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._nlp = spacy.load(\"en_core_web_sm\")\n",
    "        claucy.add_to_pipe(self._nlp)\n",
    "\n",
    "    def __call__(self, text: str) -> List[List[Tuple[str, Tuple[int, int]]]]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of response units.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, object]]: The list of segmented response units.\n",
    "            \n",
    "            Each unit is a dictionary with keys \"text\" and \"spans\":\n",
    "            The \"text\" key corresponds to the text representation, \n",
    "            The \"spans\" key corresponds to a list of character-level start and end indices (tuples) in the input text.\n",
    "        \"\"\"\n",
    "        propositions = []\n",
    "\n",
    "        for sent in self._nlp(text).sents:\n",
    "            for clause in sent._.clauses:\n",
    "                for prop in clause.to_propositions(inflect=None):\n",
    "                    propositions.append({\n",
    "                        \"text\": \" \".join([p.text for p in prop]),\n",
    "                        \"spans\": [(p.start_char, p.end_char) for p in prop]\n",
    "                    })\n",
    "\n",
    "        return propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'an old Maine man was fishing on his favorite lake',\n",
       "  'spans': [(13, 29), (30, 41), (42, 62)]},\n",
       " {'text': 'an old Maine man catching very little',\n",
       "  'spans': [(13, 29), (67, 75), (76, 87)]},\n",
       " {'text': 'he gave Finally', 'spans': [(98, 100), (101, 105), (89, 96)]},\n",
       " {'text': 'he walked back along the shore to his fishing shack',\n",
       "  'spans': [(98, 100), (113, 119), (120, 140), (141, 161)]},\n",
       " {'text': 'he walked back along the shore',\n",
       "  'spans': [(98, 100), (113, 119), (120, 140)]},\n",
       " {'text': 'he walked to his fishing shack',\n",
       "  'spans': [(98, 100), (113, 119), (141, 161)]},\n",
       " {'text': 'he got When close to the front door',\n",
       "  'spans': [(168, 170), (171, 174), (163, 167), (175, 198)]},\n",
       " {'text': 'he got close to the front door',\n",
       "  'spans': [(168, 170), (171, 174), (175, 198)]},\n",
       " {'text': 'he got When', 'spans': [(168, 170), (171, 174), (163, 167)]},\n",
       " {'text': 'he saw was', 'spans': [(200, 202), (203, 206), (210, 213)]},\n",
       " {'text': 'it was open', 'spans': [(207, 209), (210, 213), (214, 218)]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter = ClausiePropositionSegmenter()\n",
    "\n",
    "text = \"One fine day an old Maine man was fishing on his favorite lake and catching very little. Finally, he gave up and walked back along the shore to his fishing shack. When he got close to the front door, he saw it was open. \"\n",
    "\n",
    "result = segmenter(text)\n",
    "\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTRL44 Sentence Simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from spacy.tokens.span import Span\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger('sentence_transformers').setLevel(logging.ERROR)\n",
    "\n",
    "def map_segments_to_original_text(\n",
    "        original_text: str, segments: List[str], start: int,\n",
    "        model_name: str = \"sentence-transformers/paraphrase-distilroberta-base-v1\") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Uses maximum similarity matching to map the segment to a span the original sentence.\n",
    "\n",
    "    Parameters:\n",
    "        original_text (str): The original sentence.\n",
    "        segments (List[Span]): The list of segments of the original sentence.\n",
    "        model_name (str): The name of the sentence embedding model.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: The list of character-level start and end indices (tuples) in the original sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = SentenceTransformer(model_name)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    original_doc = nlp(original_text)\n",
    "\n",
    "    all_segments = []\n",
    "    for i in range(len(original_text)):\n",
    "        for j in range(i + 1, len(original_doc) + 1):\n",
    "            all_segments.append(original_doc[i:j])\n",
    "\n",
    "    segment_embeddings = np.array(model.encode([s.text for s in all_segments]))\n",
    "    simplified_sentence_embeddings = np.array(model.encode(segments))\n",
    "    similarity_matrix = cosine_similarity(simplified_sentence_embeddings, segment_embeddings)\n",
    "\n",
    "    # Find the most similar segment for each simplified sentence\n",
    "    most_similar_segment_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    simplified_to_original_mapping = [all_segments[index] for index in most_similar_segment_indices]\n",
    "\n",
    "    return [(s.start_char + start, s.end_char + start) for s in simplified_to_original_mapping]\n",
    "\n",
    "class SimpleSentenceSegmenter(Segmenter):\n",
    "    \"\"\"\n",
    "    Implementation of the Segmenter abstract base class that recursively segments text into sentences using spacy,\n",
    "    and generates the simplest possible sentences using the ctrl44-simp model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"liamcripwell/ctrl44-simp\", depth_limit=1, sep_type=\"<ssplit>\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.depth_limit = depth_limit\n",
    "        self.sep_type = sep_type\n",
    "\n",
    "    def __call__(self, text: str) -> List[Dict[str, object]]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of response units.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, object]]: The list of segmented response units.\n",
    "            \n",
    "            Each unit is a dictionary with keys \"text\" and \"spans\":\n",
    "            The \"text\" key corresponds to the text representation, \n",
    "            The \"spans\" key corresponds to a list of character-level start and end indices (tuples) in the input text.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        for sentence in self.nlp(text).sents:\n",
    "            simplified_sentences = self._simplify_sentence_recursive(sentence, depth=1)\n",
    "            spans = map_segments_to_original_text(sentence.text, [s.text for s in simplified_sentences], start=sentence.start_char)\n",
    "\n",
    "            segments.extend([{\"text\": s.text, \"spans\": [spans[i]]} for i, s in enumerate(simplified_sentences)])\n",
    "\n",
    "        return segments\n",
    "\n",
    "    def _simplify_sentence_recursive(self, sentence: Span, depth) -> List[Span]:\n",
    "        \"\"\"\n",
    "        Recursively simplifies the input sentence using the specfied model.\n",
    "\n",
    "        Parameters:\n",
    "            sentence (Span): The input sentence to be simplified.\n",
    "            depth (int): The current depth of the recursion.\n",
    "\n",
    "        Returns:\n",
    "            List[Span]: The list of simplified sentences.\n",
    "        \"\"\"\n",
    "\n",
    "        # probes model to produce a string containing at least one sentence\n",
    "        inputs = self.tokenizer(self.sep_type + sentence.text, return_tensors=\"pt\")\n",
    "        outputs = self.model.generate(**inputs, num_beams=10, max_length=128)\n",
    "        output_string = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # splits the output string into sentences\n",
    "        generated_sentences = [sent for sent in self.nlp(output_string).sents if sent.text.upper().isupper()]\n",
    "\n",
    "        # if the output string contains only one sentence, or the sentence is unchanged, return the sentence\n",
    "        if len(generated_sentences) == 1 or sentence.text in [sent.text for sent in generated_sentences]:\n",
    "            return [sentence]\n",
    "        elif depth == self.depth_limit:\n",
    "            return generated_sentences\n",
    "        else:\n",
    "            simplest_sentences = []\n",
    "            for generated_sentence in generated_sentences:\n",
    "                simplest_sentences.extend(self._simplify_sentence_recursive(generated_sentence, depth=depth+1))\n",
    "            return simplest_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ' One fine day an old Maine man was fishing on his favorite lake.',\n",
       "  'spans': [(0, 62)]},\n",
       " {'text': 'He caught very little.', 'spans': [(63, 88)]},\n",
       " {'text': ' Finally, he gave up.', 'spans': [(89, 108)]},\n",
       " {'text': 'He walked back along the shore to his fishing shack.',\n",
       "  'spans': [(109, 162)]},\n",
       " {'text': 'When he got close to the front door.', 'spans': [(163, 199)]},\n",
       " {'text': 'He saw it was open.', 'spans': [(200, 219)]},\n",
       " {'text': ' Being of a suspicious nature.', 'spans': [(220, 248)]},\n",
       " {'text': 'He walked to the door quietly and looked inside.',\n",
       "  'spans': [(250, 298)]},\n",
       " {'text': 'There was a big black bear.', 'spans': [(299, 326)]},\n",
       " {'text': ' It was just pulling the cork out of his molasses jug.',\n",
       "  'spans': [(327, 379)]},\n",
       " {'text': 'with its teeth.', 'spans': [(380, 395)]},\n",
       " {'text': ' The molasses spilled all over the floor.', 'spans': [(396, 439)]},\n",
       " {'text': 'The bear rubbed his paw in it, smearing it all over.',\n",
       "  'spans': [(440, 492)]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"One fine day an old Maine man was fishing on his favorite lake and catching very little. Finally, he gave up and walked back along the shore to his fishing shack. When he got close to the front door, he saw it was open. Being of a suspicious nature, he walked to the door quietly and looked inside. There was a big black bear. It was just pulling the cork out of his molasses jug with its teeth. The molasses spilled all over the floor and the bear rubbed his paw in it, smearing it all over.\n",
    "\"\"\"\n",
    "\n",
    "segmenter = SimpleSentenceSegmenter(depth_limit=1)\n",
    "result = segmenter.__call__(text)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OIE6 Proposition Segmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "def run_command_in_conda_env(conda_env, command, working_dir=None):\n",
    "    conda_path = os.environ.get(\"CONDA_PREFIX\")\n",
    "    if conda_path is None:\n",
    "        raise ValueError(\"Conda path not found. Please ensure you have installed Conda properly.\")\n",
    "        \n",
    "    if os.name == \"nt\":  # Windows\n",
    "        activate_command = f\"{conda_path}\\\\Scripts\\\\activate.bat {conda_env} && {command}\"\n",
    "    else:  # POSIX\n",
    "        activate_command = f\"conda activate {conda_env} && {command}\"\n",
    "    \n",
    "    process = subprocess.run(activate_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, cwd=working_dir)\n",
    "    \n",
    "    if process.returncode != 0:\n",
    "        print(f\"An error occurred while executing the command in the conda environment '{conda_env}':\")\n",
    "        print(process.stderr)\n",
    "        return None\n",
    "    \n",
    "    return process.stdout\n",
    "\n",
    "class OIE6PropositionSegmenter(Segmenter):\n",
    "    def __init__(self, working_dir=\"D:/openie6\", conda_env=\"openie6\"):\n",
    "        self._nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.conda_env = \"openie6\"\n",
    "        self.command = (\n",
    "            \"python run.py --mode predict --inp sentences.txt --out predictions.txt --rescoring --task oie --gpus 0 \"\n",
    "            \"--oie_model models/oie_model/epoch=14_eval_acc=0.551_v0.ckpt \"\n",
    "            \"--conj_model models/conj_model/epoch=28_eval_acc=0.854.ckpt \"\n",
    "            \"--rescore_model models/rescore_model\"\n",
    "        )\n",
    "        self.working_dir = working_dir\n",
    "\n",
    "    def __call__(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Splits the input text into a list of sentences using the SpaCy library and\n",
    "        further segments each sentence by proposition using the OpenIE4 library.\n",
    "\n",
    "        Parameters:\n",
    "            text (str): The input text to be segmented.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The list of segmented propositions as response units.\n",
    "        \"\"\"\n",
    "        doc = self._nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        with open(os.path.join(self.working_dir, \"sentences.txt\"), \"w\") as f:\n",
    "            f.write(\"\\n\".join(sentences))\n",
    "\n",
    "        run_command_in_conda_env(self.conda_env, self.command, self.working_dir)\n",
    "\n",
    "        with open(os.path.join(self.working_dir, \"predictions.txt.oie\"), \"r\") as f:\n",
    "            output = f.read().strip().split('\\n\\n')\n",
    "\n",
    "        propositions = []\n",
    "        for sentence_output in output:\n",
    "            sentence_propositions = sentence_output.split('\\n')\n",
    "            if len(sentence_propositions) == 1:\n",
    "                if len(sentence_propositions[0].strip()) > 0:\n",
    "                    propositions.append(sentence_propositions[0].strip())\n",
    "                continue\n",
    "            for line in sentence_propositions[1:]:\n",
    "                proposition = line[len('1.00: ('):-1].strip() #.replace('; ', '...'))\n",
    "                if len(proposition) > 0:\n",
    "                    propositions.append(proposition)\n",
    "                \n",
    "        return propositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|slow\n",
    "segmenter = OIE6PropositionSegmenter()\n",
    "segmenter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
