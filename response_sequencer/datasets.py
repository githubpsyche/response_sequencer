# AUTOGENERATED! DO NOT EDIT! File to edit: ../src/library/00_Loading_Data.ipynb.

# %% auto 0
__all__ = ['Dataset', 'SensesDataset', 'SBS_NarrativeDataset', 'JSON_Dataset']

# %% ../src/library/00_Loading_Data.ipynb 4
#| code-summary: specify the abstract class for datasets
from abc import ABC, abstractmethod
from typing import List, Dict

class Dataset(ABC):
    """
    `Dataset` is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:

    - `__len__` so that len(dataset) returns the size of the dataset.
    - `__getitem__` to support the indexing such that `dataset[i]` can be used to get ith sample
    """

    @abstractmethod
    def __len__(self
                )->int: # number of samples in the dataset
        """
        Returns the number of samples in the dataset.
        """
        pass

    @abstractmethod
    def __getitem__(self, idx: int) -> List[Dict]:
        """
        Returns a sample from the dataset.
        """
        pass

# %% ../src/library/00_Loading_Data.ipynb 10
#| code-summary: specify the Senses dataset
import numpy as np
import chardet
import hdf5storage

class SensesDataset(Dataset):
    """
    Dataset class for the Senses dataset.
    """

    def __init__(self, hdf5_file_path, sense_pool_path):
        """
        Initialize the dataset.

        Args:
            hdf5_file_path (str): Path to the HDF5 file.
            sense_pool_path (str): Path to the sense pool file.
            transform (callable, optional): Optional transform to be applied
                on a sample. Defaults to None.
        """
        # Load the data from the specified HDF5 file
        # You can customize this part to load your data
        self.data = hdf5storage.read(path='/data', filename=hdf5_file_path)

        self.trial_count = len(self.data["subject"])
        self.trial_indices = np.arange(self.trial_count)
        with open(sense_pool_path, mode='rb') as f:
            raw_data = f.read()
            detected_encoding = chardet.detect(raw_data)['encoding']
            self.sense_pool = raw_data.decode(str(detected_encoding)).split('\n')

    def __len__(self):
        """
        Return the total number of samples in the dataset.

        Returns:
            int: Number of trials in the dataset.
        """
        return self.trial_count

    def __getitem__(self, idx):
        """
        Get a trial from the dataset at the specified index.

        Args:
            trial_idx (int): Index of the trial to retrieve.

        Returns:
            sample: The retrieved sample.
        """

        # pres_itemids selects the indices from sense_pool of the target items
        trials = []
        for trial_idx in np.atleast_1d(self.trial_indices[idx]):
            senses = self.data['pres_itemids'][trial_idx]
            target_items = [self.sense_pool[each-1].strip() for each in senses if each != 0]
            
            # response units are the segments of the transcript selected by raters and their spans in the transcript
            response_transcript = str(self.data['recall_transcript'][trial_idx][0])
            response_units = [str(each) for each in self.data['response_units'][trial_idx] if str(each) != '']

            # full response units include text *and* span representations contained in a dict
            response_start_spans = [each-1 for each in self.data["response_unit_start"][trial_idx] if each != 0]
            response_end_spans = [each-1 for each in self.data["response_unit_end"][trial_idx] if each != 0]
            full_response_units = [{'text': unit, 'spans':[(start_span, end_span)]} for unit, start_span, end_span in zip(
                response_units, response_start_spans, response_end_spans)]
            
            # matchings
            match_matrix = np.zeros((len(target_items), len(response_units)), dtype=bool)
            target_indices = np.array([each-1 for each in self.data['recalls'][trial_idx] if each != 0])

            if len(target_indices) > 0:
                match_matrix[target_indices, np.arange(len(response_units))] = True

            trials.append({
                'target_context': '', 
                'target_items': target_items, 
                'response_transcript': response_transcript, 
                'response_units': full_response_units, 
                'matches': match_matrix.tolist()})
            
        return trials[0] if len(trials) == 1 else trials

# %% ../src/library/00_Loading_Data.ipynb 14
#| code-summary: specify the SBS narrative dataset
import numpy as np
import json
import os

class SBS_NarrativeDataset(Dataset):
    """
    Dataset class for the narrative free recall dataset provided by Sarah Brown-Schmidt's Conversation lab.
    """

    def __init__(self, data_directory):
        """
        Initializes the dataset.

        Args:
            data_directory (str): the directory containing the dataset files
        """

        # each response file represents a sample; these are named based on '{story_name}_{subject_id}_{iteration}'
        self.data_directory = data_directory
        self.text_directory = os.path.join(data_directory, 'texts')
        self.sequence_directory = os.path.join(data_directory, 'sequences', 'human')
        self.response_files = []
        self.stories = {}
        for path, _, files in os.walk(self.text_directory):
            for name in files:
                if name.count('_') == 2:
                    self.response_files.append(name)
                else:
                    with open(os.path.join(path, name), 'r', encoding='utf-8') as f:
                        self.stories[name[:-4]] = f.read()

        self.trial_indices = np.arange(len(self.response_files))

    def _retrieve_story_text(self, response_file_name):
        """
        Retrieves the story text for the specified response transcript.
        """
        story_name = response_file_name.split('_')[0]
        return self.stories[story_name]

    def _retrieve_response_text(self, response_file_name):
        """
        Retrieves the response text for the specified response transcript.
        """
        story_name = response_file_name.split('_')[0]
        with open(os.path.join(self.text_directory, story_name, response_file_name), 'r') as f:
            return f.read()
        
    def _retrieve_response_sequence(self, response_file_name):
        """
        Retrieves the response sequence coded for the specified response transcript.
        """
        story_name = response_file_name.split('_')[0]
        with open(os.path.join(self.sequence_directory, story_name, response_file_name[:-3]+'json'), 'r') as f:
            return json.load(f)
        
    def _prepare_match_matrix(self, response_sequence):
        """
        Prepares a match matrix based on the specified response sequence.
        """
        matchings = response_sequence['correspondences']
        match_matrix = np.zeros(
            (len(response_sequence['source_units']), len(response_sequence['response_units'])), dtype=bool)

        for response_index, matched_target in enumerate(matchings):
            if matched_target > -1:
                match_matrix[matched_target, response_index] = True
        return match_matrix.tolist()
    
    def _prepare_target_items(self, story_text, response_sequence):
        """
        Prepares a list of target items based on the specified response sequence and story text.
        """
        updated_unit_start = [story_text.find(unit) for unit in response_sequence['source_units']]
        
        for index, unit in enumerate(response_sequence['source_units']):
            assert(updated_unit_start[index] > -1)

        return [{'text': unit, 'spans':[(start, start+len(unit))]} for unit, start in zip(
            response_sequence['source_units'], updated_unit_start)]
    
    def _prepare_response_units(self, response_text, response_sequence):
        """
        Prepares a list of response units based on the specified response sequence and response text.
        """
        updated_text = [
            response_text[span[0]:span[1]].strip() for span in response_sequence['response_spans']]

        return [{'text': unit, 'spans':[(span[0], span[0]+len(unit))]} for unit, span in zip(
            updated_text, response_sequence['response_spans'])]

    def __len__(self):
        """
        Returns the number of samples in the dataset.
        """
        return len(self.response_files)
    
    def __getitem__(self, idx):
        """
        Returns a sample from the dataset.
        """
        
        trials = []
        for trial_idx in np.atleast_1d(self.trial_indices[idx]):

            response_file_name = self.response_files[trial_idx]
            story_text = self._retrieve_story_text(response_file_name)
            response_text = self._retrieve_response_text(response_file_name)
            response_sequence = self._retrieve_response_sequence(response_file_name)

            trials.append({
                'target_context': story_text, 
                'target_items': self._prepare_target_items(story_text, response_sequence), 
                'response_transcript': response_text, 
                'response_units': self._prepare_response_units(response_text, response_sequence), 
                'matches': self._prepare_match_matrix(response_sequence)})
            
        return trials[0] if len(trials) == 1 else trials

# %% ../src/library/00_Loading_Data.ipynb 18
#| code-summary: specify a concrete concrete interface for JSON datasets
import json

class JSON_Dataset(Dataset):

    def __init__(self, file_path: str):
        """
        Initializes the dataset.

        Args:
            file_path (str): the path to the JSON file containing the dataset
        """
        self.file_path = file_path
        with open(file_path, 'r') as f:
            self.data = json.load(f)

    def __len__(self
                )->int: # number of samples in the dataset
        """
        Returns the number of samples in the dataset.
        """
        return len(self.data)

    def __getitem__(self, idx: int) -> List[Dict]:
        """
        Returns a sample from the dataset.

        Each sample is a dictionary containing:
        - 'trial': the index from the reference dataset that the sample was derived from
        - 'target_context': The string containing the context of the target items. (if applicable)
        - 'target_items': The list of target items
        - 'response_transcript': The input text
        - 'response_units': The list of response units, a dictionary of the form {'text': str, 'span' [(start, end)]}
        - 'matches': a list of lists representing a 2-D boolean matrix array of shape (len(target_items), len(response_units)) containing True if the target item matches the response unit at the corresponding index.
        """
        return self.data[idx]
