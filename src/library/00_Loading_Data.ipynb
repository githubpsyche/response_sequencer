{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formatting data for use with a new library can be a time consuming process. \n",
    "If you're using multiple libraries with different data requirements, you may find yourself reformatting the same data multiple times, storing multiple copies of the same data in different formats, and struggling to synchronize changes between them.\n",
    "Rather than requiring users to reformat their data to fit the library, we we borrow an approach from [Pytorch](https://pytorch.org/).\n",
    "Instead of requiring data be specified in a specific way, Pytorch provides an abstract Dataset class that can be extended to provide a common interface for loading data from a variety of sources. \n",
    "Instead of generating the dataset in a new format, the job becomes to provide functions retrieve data samples from the original source and return them in the appropriate format.\n",
    "\n",
    "Here we outline this approach in more detail (often cribbing from the Pytorch documentation) and provide examples of how we have used it to load data in our own research.\n",
    "More specific instructions about what information a Dataset should return to work with applicable functions our library will be provided in their specific documentation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify the abstract class for datasets\n",
    "\n",
    "#| export\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict\n",
    "\n",
    "class Dataset(ABC):\n",
    "    \"\"\"\n",
    "    `Dataset` is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "    - `__len__` so that len(dataset) returns the size of the dataset.\n",
    "    - `__getitem__` to support the indexing such that `dataset[i]` can be used to get ith sample\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self\n",
    "                )->int: # number of samples in the dataset\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def __getitem__(self, idx: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a toy example, the following class specifies the interface for a dataset that accesses a text file and treats each line as a separate sample to be represented as a Markdown object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify a toy concrete class for text-based datasets\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path: str):\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            self.data = f.read().split('\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.count('\\n')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to consider lines of this project's README."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "This project explores the potential of automatic n\n"
     ]
    }
   ],
   "source": [
    "#| code-summary: demonstrate how to use the toy concrete class\n",
    "\n",
    "readme_dataset = TextDataset('../../README.md')\n",
    "print(len(readme_dataset))\n",
    "print(readme_dataset[0][:50])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senses Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify the Senses dataset\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import chardet\n",
    "import hdf5storage\n",
    "\n",
    "class SensesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the Senses dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hdf5_file_path, sense_pool_path):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "\n",
    "        Args:\n",
    "            hdf5_file_path (str): Path to the HDF5 file.\n",
    "            sense_pool_path (str): Path to the sense pool file.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Load the data from the specified HDF5 file\n",
    "        # You can customize this part to load your data\n",
    "        self.data = hdf5storage.read(path='/data', filename=hdf5_file_path)\n",
    "\n",
    "        self.trial_count = len(self.data[\"subject\"])\n",
    "        self.trial_indices = np.arange(self.trial_count)\n",
    "        with open(sense_pool_path, mode='rb') as f:\n",
    "            raw_data = f.read()\n",
    "            detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "            self.sense_pool = raw_data.decode(str(detected_encoding)).split('\\n')\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of trials in the dataset.\n",
    "        \"\"\"\n",
    "        return self.trial_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a trial from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            trial_idx (int): Index of the trial to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            sample: The retrieved sample.\n",
    "        \"\"\"\n",
    "\n",
    "        # pres_itemids selects the indices from sense_pool of the target items\n",
    "        trials = []\n",
    "        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n",
    "            senses = self.data['pres_itemids'][trial_idx]\n",
    "            target_items = [self.sense_pool[each-1].strip() for each in senses if each != 0]\n",
    "            \n",
    "            # response units are the segments of the transcript selected by raters and their spans in the transcript\n",
    "            response_transcript = str(self.data['recall_transcript'][trial_idx][0])\n",
    "            response_units = [str(each) for each in self.data['response_units'][trial_idx] if str(each) != '']\n",
    "\n",
    "            # full response units include text *and* span representations contained in a dict\n",
    "            response_start_spans = [each-1 for each in self.data[\"response_unit_start\"][trial_idx] if each != 0]\n",
    "            response_end_spans = [each-1 for each in self.data[\"response_unit_end\"][trial_idx] if each != 0]\n",
    "            full_response_units = [{'text': unit, 'spans':[(start_span, end_span)]} for unit, start_span, end_span in zip(\n",
    "                response_units, response_start_spans, response_end_spans)]\n",
    "            \n",
    "            # matchings\n",
    "            match_matrix = np.zeros((len(target_items), len(response_units)), dtype=bool)\n",
    "            target_indices = np.array([each-1 for each in self.data['recalls'][trial_idx] if each != 0])\n",
    "\n",
    "            if len(target_indices) > 0:\n",
    "                match_matrix[target_indices, np.arange(len(response_units))] = True\n",
    "\n",
    "            trials.append({\n",
    "                'target_context': '', \n",
    "                'target_items': target_items, \n",
    "                'response_transcript': response_transcript, \n",
    "                'response_units': full_response_units, \n",
    "                'matches': match_matrix.tolist()})\n",
    "            \n",
    "        return trials[0] if len(trials) == 1 else trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For trials defining this dataset, participants were cued with a word and asked to generate all the senses of that word that they could think of.\n",
    "The dataset is organized as an HDF5 file paired with a text file containing the pool of senses that participants could generate.\n",
    "    \n",
    "Each item returned by this dataset is a dictionary with the following keys:\n",
    "\n",
    "- `target_context`: the story text \n",
    "- `target_items`: a list of dictionaries, each representing a target item in the story. \n",
    "- `response_transcript`: the response transcript  \n",
    "- `response_units`: a list of dictionaries, each representing a response unit in the response transcript.\n",
    "- `matches`: a boolean matrix indicating which response units match which target items\n",
    "\n",
    "Each dictionary specified by a target item or response unit has the following keys:  \n",
    "\n",
    "- `text`: the text of the target item  \n",
    "- `spans`: a list of tuples, each representing a span of the target item in the story text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Um, as a verb, it is, like, to stop living. Um, you, it's also, like, um,\",\n",
       "  'spans': [(0, 73)]},\n",
       " {'text': 'dice is the plural of die, so, like, a six-sided object with different number of dots on each side. Um,',\n",
       "  'spans': [(74, 177)]},\n",
       " {'text': 'D-Y-E dye is like, ink to add color to something. Um. . .',\n",
       "  'spans': [(178, 235)]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-summary: load the Senses dataset\n",
    "\n",
    "import os\n",
    "\n",
    "section_tag = 'base' # unique identifier for this variation of notebook parameters\n",
    "output_dir = 'C:/Users/gunnj/workspace/response_sequencer/data/'\n",
    "\n",
    "dataset = SensesDataset(os.path.join(output_dir, f'{section_tag}_senses.h5'), os.path.join(output_dir, f'{section_tag}_sense_pool.txt'))\n",
    "\n",
    "dataset[10]['response_units']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBS Narrative Recall Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify the SBS narrative dataset\n",
    "\n",
    "#| export\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "class SBS_NarrativeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for the narrative free recall dataset provided by Sarah Brown-Schmidt's Conversation lab.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_directory):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            data_directory (str): the directory containing the dataset files\n",
    "        \"\"\"\n",
    "\n",
    "        # each response file represents a sample; these are named based on '{story_name}_{subject_id}_{iteration}'\n",
    "        self.data_directory = data_directory\n",
    "        self.text_directory = os.path.join(data_directory, 'texts')\n",
    "        self.sequence_directory = os.path.join(data_directory, 'sequences', 'human')\n",
    "        self.response_files = []\n",
    "        self.stories = {}\n",
    "        for path, _, files in os.walk(self.text_directory):\n",
    "            for name in files:\n",
    "                if name.count('_') == 2:\n",
    "                    self.response_files.append(name)\n",
    "                else:\n",
    "                    with open(os.path.join(path, name), 'r', encoding='utf-8') as f:\n",
    "                        self.stories[name[:-4]] = f.read()\n",
    "\n",
    "        self.trial_indices = np.arange(len(self.response_files))\n",
    "\n",
    "    def _retrieve_story_text(self, response_file_name):\n",
    "        \"\"\"\n",
    "        Retrieves the story text for the specified response transcript.\n",
    "        \"\"\"\n",
    "        story_name = response_file_name.split('_')[0]\n",
    "        return self.stories[story_name]\n",
    "\n",
    "    def _retrieve_response_text(self, response_file_name):\n",
    "        \"\"\"\n",
    "        Retrieves the response text for the specified response transcript.\n",
    "        \"\"\"\n",
    "        story_name = response_file_name.split('_')[0]\n",
    "        with open(os.path.join(self.text_directory, story_name, response_file_name), 'r') as f:\n",
    "            return f.read()\n",
    "        \n",
    "    def _retrieve_response_sequence(self, response_file_name):\n",
    "        \"\"\"\n",
    "        Retrieves the response sequence coded for the specified response transcript.\n",
    "        \"\"\"\n",
    "        story_name = response_file_name.split('_')[0]\n",
    "        with open(os.path.join(self.sequence_directory, story_name, response_file_name[:-3]+'json'), 'r') as f:\n",
    "            return json.load(f)\n",
    "        \n",
    "    def _prepare_match_matrix(self, response_sequence):\n",
    "        \"\"\"\n",
    "        Prepares a match matrix based on the specified response sequence.\n",
    "        \"\"\"\n",
    "        matchings = response_sequence['correspondences']\n",
    "        match_matrix = np.zeros(\n",
    "            (len(response_sequence['source_units']), len(response_sequence['response_units'])), dtype=bool)\n",
    "\n",
    "        for response_index, matched_target in enumerate(matchings):\n",
    "            if matched_target > -1:\n",
    "                match_matrix[matched_target, response_index] = True\n",
    "        return match_matrix.tolist()\n",
    "    \n",
    "    def _prepare_target_items(self, story_text, response_sequence):\n",
    "        \"\"\"\n",
    "        Prepares a list of target items based on the specified response sequence and story text.\n",
    "        \"\"\"\n",
    "        updated_unit_start = [story_text.find(unit) for unit in response_sequence['source_units']]\n",
    "        \n",
    "        for index, unit in enumerate(response_sequence['source_units']):\n",
    "            assert(updated_unit_start[index] > -1)\n",
    "\n",
    "        return [{'text': unit, 'spans':[(start, start+len(unit))]} for unit, start in zip(\n",
    "            response_sequence['source_units'], updated_unit_start)]\n",
    "    \n",
    "    def _prepare_response_units(self, response_text, response_sequence):\n",
    "        \"\"\"\n",
    "        Prepares a list of response units based on the specified response sequence and response text.\n",
    "        \"\"\"\n",
    "        updated_text = [\n",
    "            response_text[span[0]:span[1]].strip() for span in response_sequence['response_spans']]\n",
    "\n",
    "        return [{'text': unit, 'spans':[(span[0], span[0]+len(unit))]} for unit, span in zip(\n",
    "            updated_text, response_sequence['response_spans'])]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.response_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "        \"\"\"\n",
    "        \n",
    "        trials = []\n",
    "        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n",
    "\n",
    "            response_file_name = self.response_files[trial_idx]\n",
    "            story_text = self._retrieve_story_text(response_file_name)\n",
    "            response_text = self._retrieve_response_text(response_file_name)\n",
    "            response_sequence = self._retrieve_response_sequence(response_file_name)\n",
    "\n",
    "            trials.append({\n",
    "                'target_context': story_text, \n",
    "                'target_items': self._prepare_target_items(story_text, response_sequence), \n",
    "                'response_transcript': response_text, \n",
    "                'response_units': self._prepare_response_units(response_text, response_sequence), \n",
    "                'matches': self._prepare_match_matrix(response_sequence)})\n",
    "            \n",
    "        return trials[0] if len(trials) == 1 else trials"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, participants perform free recall of a story they previously read.\n",
    "The dataset is organized into a directory of text files and spreadsheets that variously represent \n",
    "encoded stories, participant responses, and coding decisions made by the lab. \n",
    "\n",
    "Each item returned by this Dataset is structured similarly to those returned by the Senses Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: load the SBS narrative dataset\n",
    "\n",
    "data_directory = 'C:/Users/gunnj/compmempy/data/narrative'\n",
    "dataset = SBS_NarrativeDataset(data_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON Dataset\n",
    "To store sequences for downstream evaluation, we'll use the JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify a concrete concrete interface for JSON datasets\n",
    "\n",
    "#| export\n",
    "import json\n",
    "\n",
    "class JSON_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): the path to the JSON file containing the dataset\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "    def __len__(self\n",
    "                )->int: # number of samples in the dataset\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset.\n",
    "\n",
    "        Each sample is a dictionary containing:\n",
    "        - 'trial': the index from the reference dataset that the sample was derived from\n",
    "        - 'target_context': The string containing the context of the target items. (if applicable)\n",
    "        - 'target_items': The list of target items\n",
    "        - 'response_transcript': The input text\n",
    "        - 'response_units': The list of response units, a dictionary of the form {'text': str, 'span' [(start, end)]}\n",
    "        - 'matches': a list of lists representing a 2-D boolean matrix array of shape (len(target_items), len(response_units)) containing True if the target item matches the response unit at the corresponding index.\n",
    "        \"\"\"\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Mug can mean, um, like a, a glass with a, but with a handle on it, so like a mug that you drink out of.',\n",
       "  'spans': [[0, 103]]},\n",
       " {'text': 'Mug can, can be a criminal act where a person mugs somebody.',\n",
       "  'spans': [[104, 164]]},\n",
       " {'text': \"Um, mug can also, uh, reference a person's head.\",\n",
       "  'spans': [[165, 213]]},\n",
       " {'text': '(silence)', 'spans': [[214, 223]]}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-summary: load a generated dataset\n",
    "dataset = JSON_Dataset('../../data/generated_dataset.json')\n",
    "dataset[1]['response_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
