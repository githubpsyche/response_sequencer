{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Units\n",
    "The Scorer abstract class defines the interface for implementations that calculate scores for each possible pairing between sets of response units and of target items. Example concrete implementations of this class can utilize methods such as keyword matching, similarity measures, or machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify the abstract class followed by Scorers\n",
    "\n",
    "#| export\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Union, Dict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Scorer(ABC):\n",
    "\n",
    "    \"\"\"\n",
    "    Abstract base class for implementing scoring strategies.\n",
    "    To create a custom scorer, inherit from this class and\n",
    "    override the score method.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(self, \n",
    "               response_units: Union[List[str], List[Dict[str, object]]], \n",
    "               target_items: Union[List[str], List[Dict[str, object]]],\n",
    "               response_context: str, target_context: str) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Matches the response_units to target_items based on a specific strategy.\n",
    "\n",
    "        Args:\n",
    "            response_units (Union[List[str], List[Dict[str, object]]]): List of response units. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            target_items (Union[List[str], List[Dict[str, object]]]): List of target items. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            response_context (str): The context of the response units.\n",
    "            target_context (str): The context of the target items.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 2D numpy array containing the scores for each possible pairing.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentenceTransformer Scorer\n",
    "The SentenceTransformer Matcher is a concrete implementation of the Matcher class that uses a similarity measure drawn from the pretrained embedding models accessible via `SentenceTransformer` module of the `sentence_transformers` library. The similarity measure used is the cosine similarity between the embedding vectors retrieved for the response unit and for the target item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify the concrete SentenceTransformer Scorer class\n",
    "\n",
    "#| export\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "\n",
    "class SentenceTransformerScorer(Scorer):\n",
    "    \"\"\"\n",
    "    Concrete Scorer class that computes similarity scores between response units\n",
    "    and target items using a SentenceTransformers model. The cosine similarity\n",
    "    between sentence embeddings is used as the similarity metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentenceTransformerScorer with a specified model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the SentenceTransformers model.\n",
    "        \"\"\"\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, \n",
    "               response_units: Union[List[str], List[Dict[str, object]]], \n",
    "               target_items: Union[List[str], List[Dict[str, object]]],\n",
    "               response_context: str = '', target_context: str = '') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Matches the response_units to target_items based on a specific strategy.\n",
    "\n",
    "        Args:\n",
    "            response_units (Union[List[str], List[Dict[str, object]]]): List of response units. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            target_items (Union[List[str], List[Dict[str, object]]]): List of target items. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            response_context (str): The context of the response units. Unused.\n",
    "            target_context (str): The context of the target items. Unused.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 2D numpy array containing the scores for each possible pairing.\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(response_units[0], dict):\n",
    "            response_units = [unit[\"text\"] for unit in response_units]\n",
    "        if isinstance(target_items[0], dict):\n",
    "            target_items = [item[\"text\"] for item in target_items]\n",
    "\n",
    "        embeddings = self.model.encode(response_units + target_items)\n",
    "        response_embeddings = np.array(embeddings[:len(response_units)])\n",
    "        target_embeddings = np.array(embeddings[len(response_units):])\n",
    "        scores_matrix = cosine_similarity(target_embeddings, response_embeddings)\n",
    "        return scores_matrix\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Cross Encoder Scorer\n",
    "The `sentence_transformer` library also hosts pretrained Cross-Encoders. Cross-Encoders require the input of a text pair and output a score 0...1. They do not work for individual sentences and they don't compute embeddings for individual texts.\n",
    "\n",
    "Since outputs are generated only per pairing, Cross Encoders tend to work more slowly than embedding-based comparison approaches, which only have to be generated once per text. However, they tend to be more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify the concrete CrossEncoder Scorer class\n",
    "\n",
    "#| export\n",
    "from sentence_transformers import CrossEncoder\n",
    "from itertools import product\n",
    "\n",
    "class CrossEncoderScorer(Scorer):\n",
    "    \"\"\"\n",
    "    Concrete Scorer class that computes similarity scores between response units\n",
    "    and target items using a SentenceTransformers model. The cosine similarity\n",
    "    between sentence embeddings is used as the similarity metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Initializes the SentenceTransformerScorer with a specified model.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the CrossEncoder model.\n",
    "        \"\"\"\n",
    "        self.model = CrossEncoder(model_name)\n",
    "\n",
    "    def __call__(self, \n",
    "               response_units: Union[List[str], List[Dict[str, object]]], \n",
    "               target_items: Union[List[str], List[Dict[str, object]]],\n",
    "               response_context: str = '', target_context: str = '') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Matches the response_units to target_items based on a specific strategy.\n",
    "\n",
    "        Args:\n",
    "            response_units (Union[List[str], List[Dict[str, object]]]): List of response units. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            target_items (Union[List[str], List[Dict[str, object]]]): List of target items. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            response_context (str): The context of the response units. Unused.\n",
    "            target_context (str): The context of the target items. Unused.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 2D numpy array containing the scores for each possible pairing.\n",
    "        \"\"\"\n",
    "        if isinstance(response_units[0], dict):\n",
    "            response_units = [unit[\"text\"] for unit in response_units]\n",
    "        if isinstance(target_items[0], dict):\n",
    "            target_items = [item[\"text\"] for item in target_items]\n",
    "            \n",
    "        pairs = list(product(target_items, response_units))\n",
    "        pair_scores = self.model.predict(pairs)\n",
    "        scores_matrix = np.array([pair_scores[i:i + len(response_units)] for i in range(\n",
    "            0, len(pair_scores), len(response_units))])\n",
    "        return scores_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextualized Embedding Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify the concrete ContextualizedEmbedding Scorer class\n",
    "\n",
    "#| export\n",
    "\n",
    "from response_sequencer.utilities import find_sublist_indices\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "def find_overlapping_indices(spans: List[Tuple[int, int]], offset_mapping: List[Tuple[int, int]]) -> List[int]:\n",
    "    overlapping_indices = []\n",
    "    \n",
    "    for span_start, span_end in spans:\n",
    "        # Iterate through the offset_mapping and compare each span\n",
    "        for idx, (offset_start, offset_end) in enumerate(offset_mapping):\n",
    "            # Check if the span and the offset_mapping overlap\n",
    "            if span_start < offset_end and offset_start < span_end:\n",
    "                # Overlapping span found, add its index to the result\n",
    "                overlapping_indices.append(idx)\n",
    "                \n",
    "    return overlapping_indices\n",
    "\n",
    "class ContextualizedEmbeddingScorer(Scorer):\n",
    "    \"\"\"\n",
    "    Cosine similarity over contextualized sentence embeddings loaded using the transformers library.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"bert-base-cased\", layer_depth: int = 1):\n",
    "        \"\"\"\n",
    "        Initializes the ContextualizedEmbeddingScorer with a specified\n",
    "        response_context, model_name, and layer selection.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Name of the SentenceTransformers model.\n",
    "            layers (Optional[List[int]]): A list of layer indices to use for contextualized embedding.\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = list(range(-layer_depth, 0))\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
    "\n",
    "    def __call__(self, \n",
    "               response_units: Union[List[str], List[Dict[str, object]]], \n",
    "               target_items: Union[List[str], List[Dict[str, object]]],\n",
    "               response_context: str, target_context: str = '') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Matches the response_units to target_items based on a specific strategy.\n",
    "\n",
    "        Args:\n",
    "            response_units (Union[List[str], List[Dict[str, object]]]): List of response units. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            target_items (Union[List[str], List[Dict[str, object]]]): List of target items. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            response_context (str): The context of the response units. Unused.\n",
    "            target_context (str): The context of the target items. Unused.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 2D numpy array containing the scores for each possible pairing.\n",
    "        \"\"\"\n",
    "\n",
    "        # if target context exists, use it to generate embeddings for all target items\n",
    "        if target_context:\n",
    "            self.target_tokens = self.tokenizer.tokenize(target_context)        \n",
    "            self.target_embeddings, self.encoded_target = self._all_token_embeddings(target_context)\n",
    "            target_embeddings = np.array([\n",
    "                self._unit_embedding(\n",
    "                item, self.target_tokens, self.encoded_target, self.target_embeddings\n",
    "                    ) for item in target_items])\n",
    "            \n",
    "        # otherwise use each target item itself as their own context\n",
    "        else:\n",
    "            target_embeddings = []\n",
    "            for item in target_items:\n",
    "                if isinstance(item, dict):\n",
    "                    item = item[\"text\"]\n",
    "                self.target_tokens = self.tokenizer.tokenize(item)\n",
    "                self.target_embeddings, self.encoded_target = self._all_token_embeddings(item)\n",
    "                target_embeddings.append(self._unit_embedding(\n",
    "                    item, self.target_tokens, self.encoded_target, self.target_embeddings\n",
    "                ))\n",
    "            target_embeddings = np.array(target_embeddings)\n",
    "\n",
    "        self.response_tokens = self.tokenizer.tokenize(response_context)\n",
    "        self.response_embeddings, self.encoded_response = self._all_token_embeddings(response_context)\n",
    "        response_embeddings = np.array([\n",
    "            self._unit_embedding(\n",
    "            unit, self.response_tokens, self.encoded_response, self.response_embeddings\n",
    "            ) for unit in response_units])\n",
    "        # response_embeddings = []\n",
    "        # for unit in response_units:\n",
    "        #     if isinstance(unit, dict):\n",
    "        #         unit = unit[\"text\"]\n",
    "        #     self.response_tokens = self.tokenizer.tokenize(unit)\n",
    "        #     self.response_states, self.encoded_response = self._get_context_states(unit)\n",
    "        #     response_embeddings.append(self._contextualized_unit_embedding(\n",
    "        #         unit, self.response_tokens, self.encoded_response, self.response_states\n",
    "        #     ))\n",
    "        # response_embeddings = np.array(response_embeddings)\n",
    "\n",
    "        return cosine_similarity(target_embeddings, response_embeddings)\n",
    "\n",
    "    def _all_token_embeddings(self, context: str):\n",
    "        \"\"\"\n",
    "        Encodes the specified context and generates all corresponding token embeddings.\n",
    "\n",
    "        Args:\n",
    "            context (str): The text context for which the hidden states are required.\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_context = self.tokenizer(context, return_tensors=\"pt\", return_offsets_mapping=True)\n",
    "            output = self.model(\n",
    "                input_ids=encoded_context[\"input_ids\"], \n",
    "                #token_type_ids=encoded_context[\"token_type_ids\"], \n",
    "                attention_mask=encoded_context[\"attention_mask\"])\n",
    "        \n",
    "        token_embeddings = torch.stack(\n",
    "            [output.hidden_states[i] for i in self.layers]).sum(0).squeeze()\n",
    "        return token_embeddings, encoded_context\n",
    "    \n",
    "    def _unit_embedding(\n",
    "            self, unit: Union[List[str], List[Dict[str, object]]],\n",
    "            tokens: List[str], encoded_context, token_embeddings) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pools an embedding for the tokens associated with a specified unit.\n",
    "\n",
    "        Args:\n",
    "            unit (str): The text unit to be encoded (either response or target unit).\n",
    "            tokens (List[str]): A list of tokens corresponding to the context.\n",
    "            encoded_context: The encoded context.\n",
    "            token_embeddings: token_embeddings associated with encoded context\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(unit, dict):\n",
    "            token_indices = find_overlapping_indices(\n",
    "                unit[\"spans\"], encoded_context[\"offset_mapping\"][0][1:-1])\n",
    "            embeddings = token_embeddings[token_indices]\n",
    "            if np.isnan(embeddings.numpy()).any():\n",
    "                raise ValueError(\"The unit spans are not valid.\")\n",
    "        else:\n",
    "            embeddings = []\n",
    "            unit_tokens = self.tokenizer.tokenize(unit)\n",
    "            unit_start_indices = find_sublist_indices(tokens, unit_tokens)\n",
    "            for start_idx in unit_start_indices:\n",
    "                start_idx += 1 # Ignore the [CLS] token\n",
    "                end_idx = start_idx + len(unit_tokens) \n",
    "                embeddings.append(token_embeddings[start_idx:end_idx].mean(dim=0).numpy())\n",
    "\n",
    "        return np.mean(np.array(embeddings), axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Embedding Scorer (Deprecated)\n",
    "Currently free embedding options are around as good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-summary: specify a concrete Scorer that uses OpenAI's text embeddings\n",
    "\n",
    "#| export\n",
    "\n",
    "import requests\n",
    "from typing import Tuple\n",
    "\n",
    "class OpenAIEmbeddingScorer(Scorer):\n",
    "    \"\"\"\n",
    "    Scorer that calculates similarity scores between response_units and target_items\n",
    "    using OpenAI's text embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, api_key: str, model: str = \"text-embedding-ada-002\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            api_key (str): Your OpenAI API key.\n",
    "            api_url (str, optional): URL for OpenAI's semantic search API.\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.model = api_url\n",
    "\n",
    "    def __call__(self, \n",
    "               response_units: Union[List[str], List[Dict[str, object]]], \n",
    "               target_items: Union[List[str], List[Dict[str, object]]],\n",
    "               response_context: str = '', target_context: str = '') -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Matches the response_units to target_items based on a specific strategy.\n",
    "\n",
    "        Args:\n",
    "            response_units (Union[List[str], List[Dict[str, object]]]): List of response units. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            target_items (Union[List[str], List[Dict[str, object]]]): List of target items. \n",
    "                Each response unit can be a string or a dictionary (when both text and spans are available).\n",
    "\n",
    "            response_context (str): The context of the response units. Unused.\n",
    "            target_context (str): The context of the target items. Unused.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 2D numpy array containing the scores for each possible pairing.\n",
    "        \"\"\"\n",
    "        \n",
    "        embeddings = self.get_embeddings(response_units + target_items)\n",
    "        response_embeddings = embeddings[:len(response_units)]\n",
    "        target_embeddings = embeddings[len(response_units):]\n",
    "\n",
    "        scores = np.inner(response_embeddings, target_embeddings)\n",
    "        return scores\n",
    "\n",
    "    def get_embeddings(self, texts: List[Union[str, Dict[str, object]]]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retrieves embeddings for the given texts from OpenAI's API.\n",
    "\n",
    "        Args:\n",
    "            texts (List[Union[str, Dict[str, object]]]): List of texts or text-spans dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: A 2D numpy array containing the embeddings for each text.\n",
    "        \"\"\"\n",
    "        prompt_strings = [text[\"text\"] if isinstance(text, dict) else text for text in texts]\n",
    "        data = {\n",
    "            \"api_key\": self.api_key,\n",
    "            \"prompts\": prompt_strings,\n",
    "            \"n\": len(prompt_strings)\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.api_url, json=data)\n",
    "        response.raise_for_status()\n",
    "        embeddings = np.array([result[\"embedding\"] for result in response.json()[\"data\"]])\n",
    "\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
