[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ResponseSequencer",
    "section": "",
    "text": "response_sequencer\nThe ResponseSequencer library is a Python library for research on automatic methods for preprocessing complex free response data as the production of a sequence of target items.\nThe project examines various frameworks that break down the task of coding free response data into discrete preprocessing steps. It evaluates multiple automated techniques for performing each step, both independently and in the context of an overall preprocessing pipeline. Specifically, we compare human and computational methods for segmenting text, matching response units with target items, and identifying the sequence of target items generated in a trial.\nThe library provides and helps develop strategies for segmenting textual free response data into response units, matching response units with target items, and identifying the sequence of target items generated in a trial. The library also enables users to create and examine different approaches for assessing the performance of the preprocessing methods for their own datasets."
  },
  {
    "objectID": "paper/index.html",
    "href": "paper/index.html",
    "title": "Abstract",
    "section": "",
    "text": "This paper explores the potential of automatic natural language processing (NLP) techniques to preprocess complex free response data in a range of domains. We examine various frameworks that break down the task of coding free response data into a series of discrete preprocessing steps and evaluate various automated techniques for each step. Specifically, we compare human and computational methods for segmenting text, matching response units with target items, and identifying the sequence of target items generated in a trial. We conclude with a discussion of the challenges and opportunities for automated free response data analysis and identify future directions for research in this area."
  },
  {
    "objectID": "paper/index.html#the-potential-of-nlp-for-automating-preprocessing",
    "href": "paper/index.html#the-potential-of-nlp-for-automating-preprocessing",
    "title": "Abstract",
    "section": "The potential of NLP for automating preprocessing",
    "text": "The potential of NLP for automating preprocessing\nOver much of the work using free response data, these problems have mainly been insurmountable, with researchers either relying on manual coding of free response data or, especially for larger-scale datasets, focusing on more structured paradigms. However, a cascade of technological advances in automatic natural language processing (NLP) has raised the possibility that many aspects of the work involved in preprocessing complex free response data can be automated, with human raters perhaps focused on confirming or correcting outputs. NLP is a subfield of artificial intelligence that focuses on developing computational methods for processing and analyzing human language. NLP techniques have been used to automate various tasks in related domains, including sentiment analysis, topic modeling, and machine translation. For example, in customer feedback analysis, NLP can help organizations quickly identify common themes and sentiments from large amounts of customer feedback data. In social media analysis, NLP can help identify emerging trends and sentiments related to a particular topic. In chatbot interactions, NLP can help generate appropriate responses to a user’s questions and requests.\nWhen these technologies work well, NLP has several advantages over manual methods for preprocessing free response data.\nTechniques leveraging this technology have already been proposed for preprocessing less complex outputs, such as a free recall of word lists (for example, see). However, manual coding remains the standard approach to preprocessing complex free response data as ordered sequences of generated target units suitable for organizational analysis.\nA range of NLP techniques has been developed that may be suitable for the two core problems of preprocessing complex free response data for organizational analysis. When it comes to segmenting complex discourse structures (rather than sequences of semantically independent words),\n\n\n\n\n\n\nTODO: Above needs a review of significant examples of work attempting to automate the coding of both straightforward and more complex free-response data and outline the domains in NLP applicable to this task"
  },
  {
    "objectID": "paper/index.html#research-question-and-objectives",
    "href": "paper/index.html#research-question-and-objectives",
    "title": "Abstract",
    "section": "Research question and objectives",
    "text": "Research question and objectives\nIn this paper, we examine the suitability of NLP for preprocessing complexly structured free response data and provide a brief overview of state of the art in automated free response data analysis. We develop a framework that decomposes the task of coding complex free response data into a sequence of discrete preprocessing steps and identifies techniques that may automate these activities. Focusing on a few especially promising approaches, we evaluate these techniques for data pre-processing across a range of free-response datasets, including free recall and word sense semantic fluency. We separately compare human and computational methods for segmenting free response data into units of text, corresponding response units with target items, and identifying the ordered sequence of target items generated in a trial. We conclude by discussing challenges and opportunities for automated free response data analysis and identifying areas for future research."
  },
  {
    "objectID": "paper/01_introduction.html#the-potential-of-nlp-for-automating-preprocessing",
    "href": "paper/01_introduction.html#the-potential-of-nlp-for-automating-preprocessing",
    "title": "Introduction",
    "section": "The potential of NLP for automating preprocessing",
    "text": "The potential of NLP for automating preprocessing\nOver much of the work using free response data, these problems have mainly been insurmountable, with researchers either relying on manual coding of free response data or, especially for larger-scale datasets, focusing on more structured paradigms. However, a cascade of technological advances in automatic natural language processing (NLP) has raised the possibility that many aspects of the work involved in preprocessing complex free response data can be automated, with human raters perhaps focused on confirming or correcting outputs. NLP is a subfield of artificial intelligence that focuses on developing computational methods for processing and analyzing human language. NLP techniques have been used to automate various tasks in related domains, including sentiment analysis, topic modeling, and machine translation. For example, in customer feedback analysis, NLP can help organizations quickly identify common themes and sentiments from large amounts of customer feedback data. In social media analysis, NLP can help identify emerging trends and sentiments related to a particular topic. In chatbot interactions, NLP can help generate appropriate responses to a user’s questions and requests.\nWhen these technologies work well, NLP has several advantages over manual methods for preprocessing free response data.\nTechniques leveraging this technology have already been proposed for preprocessing less complex outputs, such as a free recall of word lists (for example, see). However, manual coding remains the standard approach to preprocessing complex free response data as ordered sequences of generated target units suitable for organizational analysis.\nA range of NLP techniques has been developed that may be suitable for the two core problems of preprocessing complex free response data for organizational analysis. When it comes to segmenting complex discourse structures (rather than sequences of semantically independent words),\n\n\n\n\n\n\nTODO: Above needs a review of significant examples of work attempting to automate the coding of both straightforward and more complex free-response data and outline the domains in NLP applicable to this task"
  },
  {
    "objectID": "paper/01_introduction.html#research-question-and-objectives",
    "href": "paper/01_introduction.html#research-question-and-objectives",
    "title": "Introduction",
    "section": "Research question and objectives",
    "text": "Research question and objectives\nIn this paper, we examine the suitability of NLP for preprocessing complexly structured free response data and provide a brief overview of state of the art in automated free response data analysis. We develop a framework that decomposes the task of coding complex free response data into a sequence of discrete preprocessing steps and identifies techniques that may automate these activities. Focusing on a few especially promising approaches, we evaluate these techniques for data pre-processing across a range of free-response datasets, including free recall and word sense semantic fluency. We separately compare human and computational methods for segmenting free response data into units of text, corresponding response units with target items, and identifying the ordered sequence of target items generated in a trial. We conclude by discussing challenges and opportunities for automated free response data analysis and identifying areas for future research.\n\n\n\n\nEricsson, K Anders. 2006. “Protocol Analysis and Expert Thought: Concurrent Verbalizations of Thinking During Experts’ Performance on Representative Tasks.” The Cambridge Handbook of Expertise and Expert Performance, 223–41.\n\n\nHealey, M Karl, Nicole M Long, and Michael J Kahana. 2019. “Contiguity in Episodic Memory.” Psychonomic Bulletin & Review 26 (3): 699–720.\n\n\nHollway, Wendy, and Tony Jefferson. 2000. Doing Qualitative Research Differently: Free Association, Narrative and the Interview Method. Sage.\n\n\nKahana, Michael J. 2020. “Computational Models of Memory Search.” Annual Review of Psychology 71: 107–38.\n\n\nKumar, Abhilasha A. 2021. “Semantic Memory: A Review of Methods, Models, and Current Challenges.” Psychonomic Bulletin & Review 28: 40–80.\n\n\nSimmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2016. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.”"
  },
  {
    "objectID": "notebooks/index.html#install",
    "href": "notebooks/index.html#install",
    "title": "response_sequencer",
    "section": "Install",
    "text": "Install\nFor maximum flexibility while tailoring the library to your own needs, we recommend installing the library in editable mode. Clone the repository locally. Navigate to its root directory in a terminal and run the following command:\npip install -e ."
  },
  {
    "objectID": "notebooks/index.html#how-to-use",
    "href": "notebooks/index.html#how-to-use",
    "title": "response_sequencer",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n\nCode\n1+1\n\n\n2"
  },
  {
    "objectID": "notebooks/00_Loading_Data.html#sbs-narrative-recall-dataset",
    "href": "notebooks/00_Loading_Data.html#sbs-narrative-recall-dataset",
    "title": "Data Loading",
    "section": "SBS Narrative Recall Dataset",
    "text": "SBS Narrative Recall Dataset\n\n\nCode\nimport numpy as np\nimport json\nfrom torch.utils.data import Dataset\nimport os\n\nclass SBS_NarrativeDataset(Dataset):\n    def __init__(self, data_directory):\n        \n        # each response file represents a sample; these are named based on '{story_name}_{subject_id}_{iteration}'\n        self.data_directory = data_directory\n        self.text_directory = os.path.join(data_directory, 'texts')\n        self.sequence_directory = os.path.join(data_directory, 'sequences', 'human')\n        self.response_files = []\n        self.stories = {}\n        for path, subdirs, files in os.walk(self.text_directory):\n            for name in files:\n                if name.count('_') == 2:\n                    self.response_files.append(name)\n                else:\n                    with open(os.path.join(path, name), 'r', encoding='utf-8') as f:\n                        self.stories[name[:-4]] = f.read()\n\n        self.trial_indices = np.arange(len(self.response_files))\n\n    def _retrieve_story_text(self, response_file_name):\n        story_name = response_file_name.split('_')[0]\n        return self.stories[story_name]\n\n    def _retrieve_response_text(self, response_file_name):\n        story_name = response_file_name.split('_')[0]\n        with open(os.path.join(self.text_directory, story_name, response_file_name), 'r') as f:\n            return f.read()\n        \n    def _retrieve_response_sequence(self, response_file_name):\n        story_name = response_file_name.split('_')[0]\n        with open(os.path.join(self.sequence_directory, story_name, response_file_name[:-3]+'json'), 'r') as f:\n            return json.load(f)\n        \n    def _prepare_match_matrix(self, response_sequence):\n        matchings = response_sequence['correspondences']\n        match_matrix = np.zeros(\n            (len(response_sequence['source_units']), len(response_sequence['response_units'])), dtype=bool)\n\n        for response_index, matched_target in enumerate(matchings):\n            if matched_target > -1:\n                match_matrix[matched_target, response_index] = True\n        return match_matrix\n    \n    def _prepare_target_items(self, story_text, response_sequence):\n        updated_unit_start = [story_text.find(unit) for unit in response_sequence['source_units']]\n        \n        for index, unit in enumerate(response_sequence['source_units']):\n            assert(updated_unit_start[index] > -1)\n\n        return [{'text': unit, 'spans':[(start, start+len(unit))]} for unit, start in zip(\n            response_sequence['source_units'], updated_unit_start)]\n    \n    def _prepare_response_units(self, response_text, response_sequence):\n        updated_text = [\n            response_text[span[0]:span[1]].strip() for span in response_sequence['response_spans']]\n\n        return [{'text': unit, 'spans':[(span[0], span[0]+len(unit))]} for unit, span in zip(\n            updated_text, response_sequence['response_spans'])]\n\n    def __len__(self):\n        return len(self.response_files)\n    \n    def __getitem__(self, idx):\n        \n        trials = []\n        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n\n            response_file_name = self.response_files[trial_idx]\n            story_text = self._retrieve_story_text(response_file_name)\n            response_text = self._retrieve_response_text(response_file_name)\n            response_sequence = self._retrieve_response_sequence(response_file_name)\n\n            trials.append({\n                'target_context': story_text, \n                'target_items': self._prepare_target_items(story_text, response_sequence), \n                'response_transcript': response_text, \n                'response_units': self._prepare_response_units(response_text, response_sequence), \n                'matches': self._prepare_match_matrix(response_sequence)})\n            \n        return trials[0] if len(trials) == 1 else trials\n\n\n\n\nCode\ndata_directory = 'C:/Users/gunnj/compmempy/data/narrative'\ndataset = SBS_NarrativeDataset(data_directory)\n\nexample_entry = dataset[0]\nexample_entry"
  },
  {
    "objectID": "notebooks/00_Loading_Data.html#senses-dataset",
    "href": "notebooks/00_Loading_Data.html#senses-dataset",
    "title": "Data Loading",
    "section": "Senses Dataset",
    "text": "Senses Dataset\n\n\nCode\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport chardet\nimport hdf5storage\n\nclass SensesDataset(Dataset):\n    def __init__(self, hdf5_file_path, sense_pool_path, transform=None):\n        \"\"\"\n        Initialize the dataset.\n\n        Args:\n            hdf5_file_path (str): Path to the HDF5 file.\n            sense_pool_path (str): Path to the sense pool file.\n            transform (callable, optional): Optional transform to be applied\n                on a sample. Defaults to None.\n        \"\"\"\n        # Load the data from the specified HDF5 file\n        # You can customize this part to load your data\n        self.data = hdf5storage.read(path='/data', filename=hdf5_file_path)\n\n        self.trial_count = len(self.data[\"subject\"])\n        self.trial_indices = np.arange(self.trial_count)\n        with open(sense_pool_path, mode='rb') as f:\n            raw_data = f.read()\n            detected_encoding = chardet.detect(raw_data)['encoding']\n            self.sense_pool = raw_data.decode(detected_encoding).split('\\n')\n        \n        # Set the transform if provided\n        self.transform = transform\n\n    def __len__(self):\n        \"\"\"\n        Return the total number of samples in the dataset.\n\n        Returns:\n            int: Number of trials in the dataset.\n        \"\"\"\n        return self.trial_count\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get a trial from the dataset at the specified index.\n\n        Args:\n            trial_idx (int): Index of the trial to retrieve.\n\n        Returns:\n            sample: The retrieved sample.\n        \"\"\"\n\n        # pres_itemids selects the indices from sense_pool of the target items\n        trials = []\n        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n            senses = self.data['pres_itemids'][trial_idx]\n            target_items = [self.sense_pool[each-1].strip() for each in senses if each != 0]\n            \n            # response units are the segments of the transcript selected by raters and their spans in the transcript\n            response_transcript = str(self.data['recall_transcript'][trial_idx][0])\n            response_units = [str(each) for each in self.data['response_units'][trial_idx] if str(each) != '']\n\n            # full response units include text *and* span representations contained in a dict\n            response_start_spans = [each-1 for each in self.data[\"response_unit_start\"][trial_idx] if each != 0]\n            response_end_spans = [each-1 for each in self.data[\"response_unit_end\"][trial_idx] if each != 0]\n            full_response_units = [{'text': unit, 'spans':[(start_span, end_span)]} for unit, start_span, end_span in zip(\n                response_units, response_start_spans, response_end_spans)]\n            \n            # matchings\n            match_matrix = np.zeros((len(target_items), len(response_units)), dtype=bool)\n            target_indices = np.array([each-1 for each in self.data['recalls'][trial_idx] if each != 0])\n\n            if len(target_indices) > 0:\n                match_matrix[target_indices, np.arange(len(response_units))] = True\n\n            trials.append({\n                'target_context': '', \n                'target_items': target_items, \n                'response_transcript': response_transcript, \n                'response_units': full_response_units, \n                'matches': match_matrix})\n            \n        return trials[0] if len(trials) == 1 else trials\n\n\n\n\nCode\nsection_tag = 'base' # unique identifier for this variation of notebook parameters\noutput_dir = 'C:/Users/gunnj/workspace/response_sequencer/data/'\n\ndataset = SensesDataset(os.path.join(output_dir, f'{section_tag}_senses.h5'), os.path.join(output_dir, f'{section_tag}_sense_pool.txt'))\n\ndataset[:2]\n\n\n\n\nCode\nimport nbdev; nbdev.nbdev_export()"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "ResponseSequencer",
    "section": "Install",
    "text": "Install\nFor maximum flexibility while tailoring the library to your own needs, we recommend installing the library in editable mode. Clone the repository locally. Navigate to its root directory in a terminal and run the following command:\npip install -e .\nTo install the library normally without need for local cloning or library tailoring, you can alternatively run the following command in your terminal, applying no other steps:\npip install git+https://github.com/githubpsyche/response_sequencer.git"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "ResponseSequencer",
    "section": "How to use",
    "text": "How to use\nThe usage is:\n\n\nCode\n1+1\n\n\n2"
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "ResponseSequencer",
    "section": "Installation",
    "text": "Installation\nFor maximum flexibility while tailoring the library to your own needs, we recommend installing the library in editable mode using pip.\nClone the repository locally. Navigate to its root directory in a terminal and run the following command:\npip install -e .\nTo install the library normally without need for local cloning or library tailoring, you can alternatively run the following command in your terminal, applying no other steps:\npip install git+https://github.com/githubpsyche/response_sequencer.git"
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "ResponseSequencer",
    "section": "Usage",
    "text": "Usage\nThis example shows you how to use a simple preprocessing pipeline provided by our library to generate a sequence of target items from a response.\n\n\nCode\nprint('hello world')\n\n\nhello world"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "ResponseSequencer",
    "section": "Getting Started",
    "text": "Getting Started\nPeruse our documentation for more information on how to use the library."
  }
]