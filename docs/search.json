[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Toward automatic preprocessing of complex free response data",
    "section": "",
    "text": "Abstract\nThis paper explores the potential of automatic natural language processing (NLP) techniques for preprocessing complex free response data in domains pertinent to psychological research. We break down the task of coding free response data into discrete preprocessing steps such as text segmentation, response unit matching, and sequence identification and examine a range of applicable technologies for performing them. Our findings reveal that compared to a baseline approach where sentences in a response are independently compared to target items for matching, a more effective sequence can be established when contextualized embeddings are used for comparing response units to target items. Furthermore, considering every possible subsequence of the response, rather than segmenting responses by sentence, significantly improves candidate matchings. We conclude with a discussion of the challenges and opportunities for automated free response data analysis and propose future research avenues to further refine these techniques."
  },
  {
    "objectID": "src/paper/01_introduction.html",
    "href": "src/paper/01_introduction.html",
    "title": "Toward automatic preprocessing of complex free response data",
    "section": "",
    "text": "Free response data consist of unstructured, open-ended answers generated by individuals responding to prompts or cues. Unlike structured data, collected through closed-ended probes that provide limited response options, research participants produce free response data in their own words, enabling additional insight into their thought processes and mental representations. In research across various fields such as psychology, sociology, and education, free response data is often analyzed qualitatively to understand participants’ complex and varied experiences, attitudes, and perspectives (Hollway and Jefferson 2000). In cognitive science, qualitative research methods are comparatively rare, but free response data is nonetheless central to the study of the mental processes underlying human behavior. Quantitative analyses of the ordering of freely generated responses and the time taken to produce them have provided key constraints for accounts of how people retrieve information from memory, make decisions, and solve problems (Kahana 2020; Ericsson 2006).\nFree response data has been particularly influential in the study of memory search. For example, in the free recall task, participants are asked to remember a list of studied items in the order they come to mind. Data from the free recall paradigm reliably exhibits a temporal contiguity effect, in which items studied near one another are more likely to be associated and retrieved near one another. Experiments and analyses confirming this effect’s time-scale invariance, automaticity, and forward-asymmetry are central to ongoing debates about the mechanisms underlying search through episodic memory (Healey, Long, and Kahana 2019). Similarly, in the semantic fluency task, participants are prompted to generate as many exemplars as possible that belong to a specified category. The order of responses in the semantic fluency task has fueled debate about whether semantic memory is typically searched through a random walk, optimal foraging, or another process (Kumar 2021).\n\nFor free response data in these and many other domains, initial steps for preprocessing free response data to support organizational analysis can be broken down into solving two problems: (1) segmenting the response into discrete units of text (“response units”) and (2) matching these units to the set of target items or features being researched. In the free recall task, for example, the text of a participant’s free recall response is segmented into units corresponding to the words or phrases that participants generate in response to the prompt. In the semantic fluency task, the text of a participant’s response is segmented into units of text corresponding to the words or phrases that participants generate in response to the prompt. In both cases, the units of text are then matched to elements in a set of target items; these are defined as the items studied in the encoding phase in the free recall task and as the pool of exemplars or features relevant to a cue in the semantic fluency task. Response units that cannot be matched to target items are discarded or considered intrusions. The result for each set of responses is a sequence of target items considered to be generated by participants in the order they appear in the sequence.\nWhile coding data in this way is straightforward enough for preprocessing small datasets, these demands forestall the use of free response paradigms in large-scale studies involving large quantities of participants and designs involving many trials per participant. Additionally, manual segmentation and correspondence can introduce bias and inconsistencies into the data, leading to inaccurate results. Preprocessing is all the more challenging in free response tasks involving the production of narratives, concepts, or autobiographical accounts where structured multi-word responses constitute the unit of scientific interest. Complex free response data such as this require sensitivity to grammatical conventions and semantic content to appropriately segment into ordered response units and correspond with target items. Standards for coding such data are necessarily vague, often requiring multiple reviewers to preprocess the same data samples to confirm interrater reliability. Even when these measures are taken, subtle differences in rater interpretation of these standards contribute to research degrees of freedom that can make it harder to interpret results and compare findings across studies (Simmons, Nelson, and Simonsohn 2016). An increasing emphasis on organizational analyses in memory research and other domains makes it especially urgent to address these limitations and develop methods for more efficient preprocessing of free response data.\n\nOver much of the work using free response data, these problems have mainly been insurmountable, with researchers either relying on manual coding of free response data or, especially for larger-scale datasets, focusing on more structured paradigms. However, a cascade of technological advances in automatic natural language processing (NLP) has raised the possibility that many aspects of the work involved in preprocessing complex free response data can be automated, with human raters perhaps focused on confirming or correcting outputs.\n\nWhen these technologies work well, NLP has several advantages over manual methods for preprocessing free response data.\nTechniques leveraging this technology have already been proposed for preprocessing less complex outputs, such as a free recall of word lists (for example, see). However, manual coding remains the standard approach to preprocessing complex free response data as ordered sequences of generated target units suitable for organizational analysis.\nA range of NLP techniques has been developed that may be suitable for the two core problems of preprocessing complex free response data for organizational analysis. When it comes to segmenting complex discourse structures (rather than sequences of semantically independent words),\n\nIn this paper, we examine the suitability of NLP for preprocessing complexly structured free response data and provide a brief overview of state of the art in automated free response data analysis. We develop a framework that decomposes the task of coding complex free response data into a sequence of discrete preprocessing steps and identifies techniques that may automate these activities. Focusing on a few especially promising approaches, we evaluate these techniques for data pre-processing across a range of free-response datasets, including free recall and word sense semantic fluency. We separately compare human and computational methods for segmenting free response data into units of text, corresponding response units with target items, and identifying the ordered sequence of target items generated in a trial. We conclude by discussing challenges and opportunities for automated free response data analysis and identifying areas for future research.\n\n\n\n\nEricsson, K Anders. 2006. “Protocol Analysis and Expert Thought: Concurrent Verbalizations of Thinking During Experts’ Performance on Representative Tasks.” The Cambridge Handbook of Expertise and Expert Performance, 223–41.\n\n\nHealey, M Karl, Nicole M Long, and Michael J Kahana. 2019. “Contiguity in Episodic Memory.” Psychonomic Bulletin & Review 26 (3): 699–720.\n\n\nHollway, Wendy, and Tony Jefferson. 2000. Doing Qualitative Research Differently: Free Association, Narrative and the Interview Method. Sage.\n\n\nKahana, Michael J. 2020. “Computational Models of Memory Search.” Annual Review of Psychology 71: 107–38.\n\n\nKumar, Abhilasha A. 2021. “Semantic Memory: A Review of Methods, Models, and Current Challenges.” Psychonomic Bulletin & Review 28: 40–80.\n\n\nSimmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2016. “False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant.”"
  },
  {
    "objectID": "src/library/00_Loading_Data.html#the-dataset-class",
    "href": "src/library/00_Loading_Data.html#the-dataset-class",
    "title": "Loading Data",
    "section": "The Dataset Class",
    "text": "The Dataset Class\n\n\nspecify the abstract class for datasets\n#| export\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict\n\nclass Dataset(ABC):\n    \"\"\"\n    `Dataset` is an abstract class representing a dataset. Your custom dataset should inherit Dataset and override the following methods:\n\n    - `__len__` so that len(dataset) returns the size of the dataset.\n    - `__getitem__` to support the indexing such that `dataset[i]` can be used to get ith sample\n    \"\"\"\n\n    @abstractmethod\n    def __len__(self\n                )->int: # number of samples in the dataset\n        \"\"\"\n        Returns the number of samples in the dataset.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def __getitem__(self, idx: int) -> List[Dict]:\n        \"\"\"\n        Returns a sample from the dataset.\n        \"\"\"\n        pass\n\n\nAs a toy example, the following class specifies the interface for a dataset that accesses a text file and treats each line as a separate sample to be represented as a Markdown object.\n\n\nspecify a toy concrete class for text-based datasets\nclass TextDataset(Dataset):\n\n    def __init__(self, dataset_path: str):\n        with open(dataset_path, 'r') as f:\n            self.data = f.read().split('\\n')\n\n    def __len__(self):\n        return self.data.count('\\n')\n    \n    def __getitem__(self, idx):\n        return self.data[idx]\n\n\nWe can use it to consider lines of this project’s README.\n\n\ndemonstrate how to use the toy concrete class\nreadme_dataset = TextDataset('../../README.md')\nprint(len(readme_dataset))\nprint(readme_dataset[0][:50])\n\n\n0\nThis project explores the potential of automatic n"
  },
  {
    "objectID": "src/library/00_Loading_Data.html#senses-dataset",
    "href": "src/library/00_Loading_Data.html#senses-dataset",
    "title": "Loading Data",
    "section": "Senses Dataset",
    "text": "Senses Dataset\n\n\nspecify the Senses dataset\n#| export\nimport numpy as np\nimport chardet\nimport hdf5storage\n\nclass SensesDataset(Dataset):\n    \"\"\"\n    Dataset class for the Senses dataset.\n    \"\"\"\n\n    def __init__(self, hdf5_file_path, sense_pool_path):\n        \"\"\"\n        Initialize the dataset.\n\n        Args:\n            hdf5_file_path (str): Path to the HDF5 file.\n            sense_pool_path (str): Path to the sense pool file.\n            transform (callable, optional): Optional transform to be applied\n                on a sample. Defaults to None.\n        \"\"\"\n        # Load the data from the specified HDF5 file\n        # You can customize this part to load your data\n        self.data = hdf5storage.read(path='/data', filename=hdf5_file_path)\n\n        self.trial_count = len(self.data[\"subject\"])\n        self.trial_indices = np.arange(self.trial_count)\n        with open(sense_pool_path, mode='rb') as f:\n            raw_data = f.read()\n            detected_encoding = chardet.detect(raw_data)['encoding']\n            self.sense_pool = raw_data.decode(str(detected_encoding)).split('\\n')\n\n    def __len__(self):\n        \"\"\"\n        Return the total number of samples in the dataset.\n\n        Returns:\n            int: Number of trials in the dataset.\n        \"\"\"\n        return self.trial_count\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Get a trial from the dataset at the specified index.\n\n        Args:\n            trial_idx (int): Index of the trial to retrieve.\n\n        Returns:\n            sample: The retrieved sample.\n        \"\"\"\n\n        # pres_itemids selects the indices from sense_pool of the target items\n        trials = []\n        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n            senses = self.data['pres_itemids'][trial_idx]\n            target_items = [self.sense_pool[each-1].strip() for each in senses if each != 0]\n            \n            # response units are the segments of the transcript selected by raters and their spans in the transcript\n            response_transcript = str(self.data['recall_transcript'][trial_idx][0])\n            response_units = [str(each) for each in self.data['response_units'][trial_idx] if str(each) != '']\n\n            # full response units include text *and* span representations contained in a dict\n            response_start_spans = [each-1 for each in self.data[\"response_unit_start\"][trial_idx] if each != 0]\n            response_end_spans = [each-1 for each in self.data[\"response_unit_end\"][trial_idx] if each != 0]\n            full_response_units = [{'text': unit, 'spans':[(start_span, end_span)]} for unit, start_span, end_span in zip(\n                response_units, response_start_spans, response_end_spans)]\n            \n            # matchings\n            match_matrix = np.zeros((len(target_items), len(response_units)), dtype=bool)\n            target_indices = np.array([each-1 for each in self.data['recalls'][trial_idx] if each != 0])\n\n            if len(target_indices) > 0:\n                match_matrix[target_indices, np.arange(len(response_units))] = True\n\n            trials.append({\n                'target_context': '', \n                'target_items': target_items, \n                'response_transcript': response_transcript, \n                'response_units': full_response_units, \n                'matches': match_matrix.tolist()})\n            \n        return trials[0] if len(trials) == 1 else trials\n\n\nFor trials defining this dataset, participants were cued with a word and asked to generate all the senses of that word that they could think of. The dataset is organized as an HDF5 file paired with a text file containing the pool of senses that participants could generate.\nEach item returned by this dataset is a dictionary with the following keys:\n\ntarget_context: the story text\ntarget_items: a list of dictionaries, each representing a target item in the story.\nresponse_transcript: the response transcript\n\nresponse_units: a list of dictionaries, each representing a response unit in the response transcript.\nmatches: a boolean matrix indicating which response units match which target items\n\nEach dictionary specified by a target item or response unit has the following keys:\n\ntext: the text of the target item\n\nspans: a list of tuples, each representing a span of the target item in the story text\n\n\n\nload the Senses dataset\nimport os\n\nsection_tag = 'base' # unique identifier for this variation of notebook parameters\noutput_dir = 'C:/Users/gunnj/workspace/response_sequencer/data/'\n\ndataset = SensesDataset(os.path.join(output_dir, f'{section_tag}_senses.h5'), os.path.join(output_dir, f'{section_tag}_sense_pool.txt'))\n\ndataset[10]['response_units']\n\n\n[{'text': \"Um, as a verb, it is, like, to stop living. Um, you, it's also, like, um,\",\n  'spans': [(0, 73)]},\n {'text': 'dice is the plural of die, so, like, a six-sided object with different number of dots on each side. Um,',\n  'spans': [(74, 177)]},\n {'text': 'D-Y-E dye is like, ink to add color to something. Um. . .',\n  'spans': [(178, 235)]}]"
  },
  {
    "objectID": "src/library/00_Loading_Data.html#sbs-narrative-recall-dataset",
    "href": "src/library/00_Loading_Data.html#sbs-narrative-recall-dataset",
    "title": "Loading Data",
    "section": "SBS Narrative Recall Dataset",
    "text": "SBS Narrative Recall Dataset\n\n\nspecify the SBS narrative dataset\n#| export\nimport numpy as np\nimport json\nimport os\n\nclass SBS_NarrativeDataset(Dataset):\n    \"\"\"\n    Dataset class for the narrative free recall dataset provided by Sarah Brown-Schmidt's Conversation lab.\n    \"\"\"\n\n    def __init__(self, data_directory):\n        \"\"\"\n        Initializes the dataset.\n\n        Args:\n            data_directory (str): the directory containing the dataset files\n        \"\"\"\n\n        # each response file represents a sample; these are named based on '{story_name}_{subject_id}_{iteration}'\n        self.data_directory = data_directory\n        self.text_directory = os.path.join(data_directory, 'texts')\n        self.sequence_directory = os.path.join(data_directory, 'sequences', 'human')\n        self.response_files = []\n        self.stories = {}\n        for path, _, files in os.walk(self.text_directory):\n            for name in files:\n                if name.count('_') == 2:\n                    self.response_files.append(name)\n                else:\n                    with open(os.path.join(path, name), 'r', encoding='utf-8') as f:\n                        self.stories[name[:-4]] = f.read()\n\n        self.trial_indices = np.arange(len(self.response_files))\n\n    def _retrieve_story_text(self, response_file_name):\n        \"\"\"\n        Retrieves the story text for the specified response transcript.\n        \"\"\"\n        story_name = response_file_name.split('_')[0]\n        return self.stories[story_name]\n\n    def _retrieve_response_text(self, response_file_name):\n        \"\"\"\n        Retrieves the response text for the specified response transcript.\n        \"\"\"\n        story_name = response_file_name.split('_')[0]\n        with open(os.path.join(self.text_directory, story_name, response_file_name), 'r') as f:\n            return f.read()\n        \n    def _retrieve_response_sequence(self, response_file_name):\n        \"\"\"\n        Retrieves the response sequence coded for the specified response transcript.\n        \"\"\"\n        story_name = response_file_name.split('_')[0]\n        with open(os.path.join(self.sequence_directory, story_name, response_file_name[:-3]+'json'), 'r') as f:\n            return json.load(f)\n        \n    def _prepare_match_matrix(self, response_sequence):\n        \"\"\"\n        Prepares a match matrix based on the specified response sequence.\n        \"\"\"\n        matchings = response_sequence['correspondences']\n        match_matrix = np.zeros(\n            (len(response_sequence['source_units']), len(response_sequence['response_units'])), dtype=bool)\n\n        for response_index, matched_target in enumerate(matchings):\n            if matched_target > -1:\n                match_matrix[matched_target, response_index] = True\n        return match_matrix.tolist()\n    \n    def _prepare_target_items(self, story_text, response_sequence):\n        \"\"\"\n        Prepares a list of target items based on the specified response sequence and story text.\n        \"\"\"\n        updated_unit_start = [story_text.find(unit) for unit in response_sequence['source_units']]\n        \n        for index, unit in enumerate(response_sequence['source_units']):\n            assert(updated_unit_start[index] > -1)\n\n        return [{'text': unit, 'spans':[(start, start+len(unit))]} for unit, start in zip(\n            response_sequence['source_units'], updated_unit_start)]\n    \n    def _prepare_response_units(self, response_text, response_sequence):\n        \"\"\"\n        Prepares a list of response units based on the specified response sequence and response text.\n        \"\"\"\n        updated_text = [\n            response_text[span[0]:span[1]].strip() for span in response_sequence['response_spans']]\n\n        return [{'text': unit, 'spans':[(span[0], span[0]+len(unit))]} for unit, span in zip(\n            updated_text, response_sequence['response_spans'])]\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of samples in the dataset.\n        \"\"\"\n        return len(self.response_files)\n    \n    def __getitem__(self, idx):\n        \"\"\"\n        Returns a sample from the dataset.\n        \"\"\"\n        \n        trials = []\n        for trial_idx in np.atleast_1d(self.trial_indices[idx]):\n\n            response_file_name = self.response_files[trial_idx]\n            story_text = self._retrieve_story_text(response_file_name)\n            response_text = self._retrieve_response_text(response_file_name)\n            response_sequence = self._retrieve_response_sequence(response_file_name)\n\n            trials.append({\n                'target_context': story_text, \n                'target_items': self._prepare_target_items(story_text, response_sequence), \n                'response_transcript': response_text, \n                'response_units': self._prepare_response_units(response_text, response_sequence), \n                'matches': self._prepare_match_matrix(response_sequence)})\n            \n        return trials[0] if len(trials) == 1 else trials\n\n\nIn this dataset, participants perform free recall of a story they previously read. The dataset is organized into a directory of text files and spreadsheets that variously represent encoded stories, participant responses, and coding decisions made by the lab.\nEach item returned by this Dataset is structured similarly to those returned by the Senses Dataset.\n\n\nload the SBS narrative dataset\ndata_directory = 'C:/Users/gunnj/compmempy/data/narrative'\ndataset = SBS_NarrativeDataset(data_directory)"
  },
  {
    "objectID": "src/library/00_Loading_Data.html#json-dataset",
    "href": "src/library/00_Loading_Data.html#json-dataset",
    "title": "Loading Data",
    "section": "JSON Dataset",
    "text": "JSON Dataset\nTo store sequences for downstream evaluation, we’ll use the JSON format.\n\n\nspecify a concrete concrete interface for JSON datasets\n#| export\nimport json\n\nclass JSON_Dataset(Dataset):\n\n    def __init__(self, file_path: str):\n        \"\"\"\n        Initializes the dataset.\n\n        Args:\n            file_path (str): the path to the JSON file containing the dataset\n        \"\"\"\n        self.file_path = file_path\n        with open(file_path, 'r') as f:\n            self.data = json.load(f)\n\n    def __len__(self\n                )->int: # number of samples in the dataset\n        \"\"\"\n        Returns the number of samples in the dataset.\n        \"\"\"\n        return len(self.data)\n\n    def __getitem__(self, idx: int) -> List[Dict]:\n        \"\"\"\n        Returns a sample from the dataset.\n\n        Each sample is a dictionary containing:\n        - 'trial': the index from the reference dataset that the sample was derived from\n        - 'target_context': The string containing the context of the target items. (if applicable)\n        - 'target_items': The list of target items\n        - 'response_transcript': The input text\n        - 'response_units': The list of response units, a dictionary of the form {'text': str, 'span' [(start, end)]}\n        - 'matches': a list of lists representing a 2-D boolean matrix array of shape (len(target_items), len(response_units)) containing True if the target item matches the response unit at the corresponding index.\n        \"\"\"\n        return self.data[idx]\n\n\n\n\nload a generated dataset\ndataset = JSON_Dataset('../../data/generated_dataset.json')\ndataset[1]['response_units']\n\n\n[{'text': 'Mug can mean, um, like a, a glass with a, but with a handle on it, so like a mug that you drink out of.',\n  'spans': [[0, 103]]},\n {'text': 'Mug can, can be a criminal act where a person mugs somebody.',\n  'spans': [[104, 164]]},\n {'text': \"Um, mug can also, uh, reference a person's head.\",\n  'spans': [[165, 213]]},\n {'text': '(silence)', 'spans': [[214, 223]]}]"
  },
  {
    "objectID": "src/analyses/Comparison.html",
    "href": "src/analyses/Comparison.html",
    "title": "Demo",
    "section": "",
    "text": "Code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_theme(style=\"darkgrid\")\n\nfor metric_index, metric_name in enumerate(metrics):\n    x = sns.pointplot(x=\"Sequencer\", y=\"score\", data=scores[metric_name], join=False)\n    x.set_ylabel(metric_name.replace('_', ' '))\n    x.set_xlabel('')\n    x.set_title(section_tag.replace('_', ' '))\n\n    plt.show()"
  },
  {
    "objectID": "src/paper/references.html",
    "href": "src/paper/references.html",
    "title": "References",
    "section": "",
    "text": "Ericsson, K Anders. 2006. “Protocol Analysis and Expert Thought:\nConcurrent Verbalizations of Thinking During Experts’ Performance on\nRepresentative Tasks.” The Cambridge Handbook of Expertise\nand Expert Performance, 223–41.\n\n\nHealey, M Karl, Nicole M Long, and Michael J Kahana. 2019.\n“Contiguity in Episodic Memory.” Psychonomic Bulletin\n& Review 26 (3): 699–720.\n\n\nHollway, Wendy, and Tony Jefferson. 2000. Doing Qualitative Research\nDifferently: Free Association, Narrative and the Interview Method.\nSage.\n\n\nKahana, Michael J. 2020. “Computational Models of Memory\nSearch.” Annual Review of Psychology 71: 107–38.\n\n\nKumar, Abhilasha A. 2021. “Semantic Memory: A Review of Methods,\nModels, and Current Challenges.” Psychonomic Bulletin &\nReview 28: 40–80.\n\n\nSimmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2016.\n“False-Positive Psychology: Undisclosed Flexibility in Data\nCollection and Analysis Allows Presenting Anything as\nSignificant.”"
  }
]